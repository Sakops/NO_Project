{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1bc4d418b6f666",
   "metadata": {},
   "source": [
    "# Basic Submission\n",
    "\n",
    "This Jupyter Notebook is designed to explore a variety of advanced optimization techniques applied to two specific functions: the well-known Rosenbrock function and a custom-designed function. This study encompasses Newton-based techniques, conjugate gradient techniques if the linear and non-linear type, quasi-Newton methods and gradient and Hessian approximation. The aim is to comprehensively compare the effectiveness and efficiency of these diverse approaches in finding global minima and understanding their behavior under different mathematical conditions and complexities by subjecting them to a representative test suite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c25103e8acca43f",
   "metadata": {},
   "source": [
    "## Coverage\n",
    "\n",
    "For each of three starting points starting points:\n",
    "\n",
    "| Problem Domain          | Rosenbrock | Custom Function | Derivative Approximation (T5) | Comment                       |\n",
    "|-------------------------|-----|----------|------------------------|-------------------------------|\n",
    "| Newton (T1)             |✅|✅|✅|                               |\n",
    "| Linear CG (T2)          |✅|✅|✅|                               |\n",
    "| Non-Linear CG: FR (T3)  |✅|✅|✅|                               |\n",
    "| Non-Linear CG: PR (T3)  |✅|✅|✅|                               |\n",
    "| Quasi-Newton: BFGS (T4) |✅|✅|✅|                               |\n",
    "| Quasi-Newton: SR1 (T4)  |✅|✅|✅| Trust Region FW _not_ covered |\n",
    "| Outperform Newton (T6)  |✅|✅|(n/a)|                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da13a0c2145e5a",
   "metadata": {},
   "source": [
    "## Theoretical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b526bb930cbf980",
   "metadata": {},
   "source": [
    "### Target Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40712df2d0007644",
   "metadata": {},
   "source": [
    "#### Rosenbrock Function\n",
    "\n",
    "The Rosenbrock function is formulated as:\n",
    "$$ f(x) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2 $$\n",
    "\n",
    "To find the global minimum analytically, we start by setting the gradient of the function to zero. The gradient components are:\n",
    "$$ \\frac{\\partial f}{\\partial x_1} = -400x_1(x_2 - x_1^2) - 2(1 - x_1) $$\n",
    "$$ \\frac{\\partial f}{\\partial x_2} = 200(x_2 - x_1^2) $$\n",
    "\n",
    "Setting these derivatives to zero, we derive:\n",
    "\n",
    "1. From $\\frac{\\partial f}{\\partial x_2} = 0$, we find that $x_2 = x_1^2$.\n",
    "2. Substituting $x_2 = x_1^2$ into $\\frac{\\partial f}{\\partial x_1}$ and simplifying, we get:\n",
    "   $$ -400x_1(x_1^2 - x_1^2) - 2(1 - x_1) = 0 $$\n",
    "   $$ -2(1 - x_1) = 0 $$\n",
    "   Leading to $x_1 = 1$.\n",
    "\n",
    "Given $x_1 = 1$, and substituting back, we find $x_2 = 1^2 = 1$.\n",
    "\n",
    "Therefore, the global minimum is at $(x_1, x_2) = (1, 1)$ where $f(x) = 0$, lying at the bottom of a long, narrow, parabolic-shaped valley.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeb4ffa7de72bf8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.384140Z",
     "start_time": "2024-05-19T15:55:35.771052Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def safe_rosenbrock(x):\n",
    "    \"\"\"Calculate the Rosenbrock function value at x.\"\"\"\n",
    "    # Clipping x values to prevent excessive values\n",
    "    x = np.clip(x, -10, 10)\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcbdde772acf9c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.385659Z",
     "start_time": "2024-05-19T15:55:35.775813Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def safe_grad_rosenbrock(x):\n",
    "    \"\"\"Calculate the gradient of the Rosenbrock function at x.\"\"\"\n",
    "    x = np.clip(x, -10, 10)\n",
    "    grad = np.zeros(2)\n",
    "    grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
    "    grad[1] = 200 * (x[1] - x[0]**2)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e78cf60a567bd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.385752Z",
     "start_time": "2024-05-19T15:55:35.779499Z"
    }
   },
   "outputs": [],
   "source": [
    "def hessian_rosenbrock(x):\n",
    "    \"\"\" Calculate the Hessian matrix of the Rosenbrock function at x. \"\"\"\n",
    "    x1, x2 = x\n",
    "    return np.array([[1200 * x1**2 - 400 * x2 + 2, -400 * x1],\n",
    "                     [-400 * x1, 200]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e742554dba87d52",
   "metadata": {},
   "source": [
    "#### Custom Function\n",
    "\n",
    "The custom function is defined as:\n",
    "$$ f(x) = 150(x_1 x_2)^2 + (0.5 x_1 + 2 x_2 - 2)^2 $$\n",
    "\n",
    "To analyze the global minimum, we set the gradient to zero. The gradient components are:\n",
    "$$ \\frac{\\partial f}{\\partial x_1} = 300 x_1 x_2^2 + (0.5 x_1 + 2 x_2 - 2) $$\n",
    "$$ \\frac{\\partial f}{\\partial x_2} = 300 x_1^2 x_2 + 4(0.5 x_1 + 2 x_2 - 2) $$\n",
    "\n",
    "Setting these derivatives to zero, we simplify and solve:\n",
    "\n",
    "1. From $\\frac{\\partial f}{\\partial x_1} = 0$ and $\\frac{\\partial f}{\\partial x_2} = 0$, assuming $x_1 = 0$, we find:\n",
    "   $$ 2 x_2 - 2 = 0 $$\n",
    "   Leading to $x_2 = 1$.\n",
    "2. Substituting $x_1 = 0$ into $\\frac{\\partial f}{\\partial x_2}$ yields:\n",
    "   $$ 4(2 x_2 - 2) = 0 $$\n",
    "   Confirms that $x_2 = 1$.\n",
    "\n",
    "Therefore, the point $(x_1, x_2) = (0, 1)$ represents a critical point. Further analysis, potentially including second derivative tests or numerical verification, would confirm its nature as a global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5176396bb825a4e",
   "metadata": {},
   "source": [
    "To determine if there is another point where $f(x_1, x_2) = 0$ for the custom function:\n",
    "$$ f(x) = 150(x_1 x_2)^2 + (0.5 x_1 + 2 x_2 - 2)^2 $$\n",
    "\n",
    "we need to find conditions under which both terms in the function evaluate to zero because this is the only way their sum can be zero. Here's the breakdown:\n",
    "\n",
    "**Analyzing Each Term for Zero:**\n",
    "\n",
    "1. **The first term $150(x_1 x_2)^2$ equals zero when:**\n",
    "   - $x_1 = 0$ or\n",
    "   - $x_2 = 0$\n",
    "\n",
    "2. **The second term $(0.5 x_1 + 2 x_2 - 2)^2$ equals zero when:**\n",
    "   - $0.5 x_1 + 2 x_2 - 2 = 0$\n",
    "\n",
    "**Solving for Conditions:**\n",
    "\n",
    "Given the conditions from the second term, we rearrange the equation:\n",
    "$$ 0.5 x_1 + 2 x_2 = 2 $$\n",
    "$$ x_2 = 1 - 0.25 x_1 $$\n",
    "\n",
    "Now, substitute this expression for $x_2$ into the condition from the first term:\n",
    "$$ x_1 \\cdot (1 - 0.25 x_1) = 0 $$\n",
    "\n",
    "This equation is satisfied when $x_1 = 0$ or $x_1 = 4$. Let's explore both:\n",
    "\n",
    "- **If $x_1 = 0$**:\n",
    "  - Substituting $x_1 = 0$ in $x_2 = 1 - 0.25 \\cdot 0$:\n",
    "  - $x_2 = 1$\n",
    "  - This point $(0, 1)$ was already identified as a global minimizer where $f(x) = 0$.\n",
    "\n",
    "- **If $x_1 = 4$**:\n",
    "  - Substituting $x_1 = 4$ in $x_2 = 1 - 0.25 \\cdot 4$:\n",
    "  - $x_2 = 0$\n",
    "  - This results in the point $(4, 0)$, and checking $f(4, 0)$:\n",
    "  - $f(4, 0) = 150(4 \\cdot 0)^2 + (0.5 \\cdot 4 + 2 \\cdot 0 - 2)^2 = 0$\n",
    "  - This shows $f(4, 0)$ is zero, too, constituting a second global minimizer candidate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc865c47b1e1e1dc",
   "metadata": {},
   "source": [
    "To verify whether $(4, 0)$ is also a minimum, we evaluate the gradient components at this point.\n",
    "\n",
    "At $(4, 0)$ we get:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x_1} = 300 \\cdot 4 \\cdot 0^2 + 0.5(0.5 \\cdot 4 + 2 \\cdot 0 - 2) = 0 $$\n",
    "$$ \\frac{\\partial f}{\\partial x_2} = 300 \\cdot 4^2 \\cdot 0 + 2(0.5 \\cdot 4 + 2 \\cdot 0 - 2) = 0 $$\n",
    "\n",
    "Both derivatives are zero, confirming that $(4, 0)$ is a critical point. As with $(0,1)$, further analysis would confirm its nature as a minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4cc21ec9631314e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.539328Z",
     "start_time": "2024-05-19T15:55:35.790174Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_function(x):\n",
    "    \"\"\"Calculate the custom function value at x.\"\"\"\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ed356632ca62fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.539470Z",
     "start_time": "2024-05-19T15:55:35.795039Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_custom_function(x):\n",
    "    \"\"\"Calculate the gradient of the custom function at x.\"\"\"\n",
    "    grad = np.zeros(2)\n",
    "    grad[0] = 300 * x[0] * x[1]**2 + 2 * (0.5 * x[0] + 2 * x[1] - 2) * 0.5\n",
    "    grad[1] = 300 * x[0]**2 * x[1] + 2 * (0.5 * x[0] + 2 * x[1] - 2) * 2\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "274aa76f5cdf9a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.543656Z",
     "start_time": "2024-05-19T15:55:35.798377Z"
    }
   },
   "outputs": [],
   "source": [
    "def hessian_custom_function(x):\n",
    "    \"\"\"Calculate the Hessian matrix of the custom function at x. \"\"\"\n",
    "    x1, x2 = x\n",
    "    return np.array([[300 * x2**2 + 0.5, 600 * x1 * x2 + 2],\n",
    "                     [600 * x1 * x2 + 2, 300 * x1**2 + 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359623f641d1ee2a",
   "metadata": {},
   "source": [
    "## Methodological Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc642b5403b47cc",
   "metadata": {},
   "source": [
    "### Line Search Methods\n",
    "\n",
    "A crucial component in optimization algorithms that use gradient information is the line search technique. The line search aims to find an acceptable step size that satisfies certain conditions, improving the convergence of the method.\n",
    "\n",
    "#### Backtracking Line Search\n",
    "\n",
    "**Description:**\n",
    "Backtracking line search is an iterative method for determining the step size in optimization algorithms. This method reduces the step size until a decrease in the function value satisfies the Armijo condition, a form of the Wolfe conditions, which are used to ensure sufficient decrease and convergence.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "Backtracking line search involves iteratively reducing the step size $\\alpha$ based on a shrinkage factor until the function value at the new point is less than the value at the current point decreased by a scaled gradient magnitude. The condition is formally given by:\n",
    "\n",
    "$$ f(x + \\alpha p) \\leq f(x) + c \\alpha \\nabla f(x)^T p $$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the step size,\n",
    "- $p$ is the search direction (typically the negative gradient),\n",
    "- $c$ is a small scalar, usually between $0.01$ and $0.3$,\n",
    "- $\\nabla f(x)$ is the gradient of the function at $x$.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Start with an initial guess for the step size, usually $\\alpha = 1$.\n",
    "2. Evaluate the function at $x + \\alpha p$.\n",
    "3. If the function value at $x + \\alpha p$ is more than $f(x) + c \\alpha \\nabla f(x)^T p$, reduce $\\alpha$ by multiplying it by a reduction factor $\\rho$ (typically around $0.5$ to $0.8$).\n",
    "4. Repeat the evaluation until the condition is met.\n",
    "5. Once the condition is satisfied, accept $\\alpha$ as the step size.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "Backtracking line search is particularly useful in non-convex optimization problems where choosing an appropriate step size is crucial for convergence. It adapts the step size based on the local landscape of the function, helping to avoid steps that are too large which may lead to divergence or overshooting the minimum.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- The initial step size and the values of $c$ and $\\rho$ are crucial parameters. $c$ should be small to ensure sufficient decrease, while $\\rho$ should be chosen to make meaningful reductions in step size.\n",
    "- It's important to balance the aggressiveness of the reduction to avoid excessive function evaluations.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Simple to implement and does not require any complex calculations beyond function evaluation and gradient computation. It is robust and can be used in conjunction with a variety of optimization algorithms.\n",
    "- **Limitations:** Can result in a large number of function evaluations, especially if the initial guess for $\\alpha$ is far from optimal. The method might also make very slow progress if the parameters are not chosen well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1510ccb601402850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.676356Z",
     "start_time": "2024-05-19T15:55:35.805444Z"
    }
   },
   "outputs": [],
   "source": [
    "def line_search(func, grad_func, x, d, alpha_init=1.0, rho=0.9, c=1e-4, max_iter=50):\n",
    "    \"\"\"\n",
    "    Conducts a backtracking line search to find the step size that satisfies the Armijo condition.\n",
    "    \n",
    "    This line search method reduces the step size alpha iteratively until a decrease in the function\n",
    "    value satisfies the Armijo condition, which ensures sufficient decrease.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The objective function to minimize. It should take a single numpy array argument.\n",
    "        grad_func (callable): The gradient of the objective function. It should take a single numpy array argument.\n",
    "        x (np.array): The current point in the parameter space where the function is evaluated.\n",
    "        d (np.array): The current search direction along which the line search is performed.\n",
    "        alpha_init (float): The initial step size for the line search.\n",
    "        rho (float): The factor by which the step size is reduced in each iteration (0 < rho < 1).\n",
    "        c (float): The Armijo constant used in the sufficient decrease condition (0 < c < 1).\n",
    "        max_iter (int): The maximum number of iterations to perform if the Armijo condition is not met.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - alpha (float): The step size that satisfies the Armijo condition or the step size at the end of max_iter iterations.\n",
    "            - log (list of dicts): A log of each iteration's details including the iteration number, alpha value, function value, and target Armijo condition value.\n",
    "    \"\"\"\n",
    "    alpha = alpha_init\n",
    "    log = []\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        f_current = func(x)\n",
    "        grad_current = grad_func(x)\n",
    "        f_test = func(x + alpha * d)\n",
    "        armijo_condition = f_current + c * alpha * np.dot(grad_current, d)\n",
    "        log.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'alpha': alpha,\n",
    "            'function_value': f_test,\n",
    "            'target_value': armijo_condition\n",
    "        })\n",
    "#        if f_test <= armijo_condition or iteration >= max_iter:\n",
    "#            break\n",
    "        if f_test <= armijo_condition:\n",
    "            break\n",
    "        alpha *= rho\n",
    "        iteration += 1\n",
    "    return alpha, log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc8ff91578bc5e",
   "metadata": {},
   "source": [
    "### Optimization Algorithms: Newton Method with Hessian Modification\n",
    "\n",
    "**Description:**\n",
    "Newton's method is a second-order optimization algorithm that uses the Hessian matrix of second derivatives to find the zeros of a function, or the local maxima and minima. This version includes a modification to ensure the Hessian matrix is always positive definite, enhancing stability and convergence.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "Given an objective function $f(x)$, Newton's method updates the current point $x$ according to:\n",
    "\n",
    "$$ x_{\\text{new}} = x_{\\text{old}} - H^{-1}(x_{\\text{old}}) \\nabla f(x_{\\text{old}}) $$\n",
    "\n",
    "where $H(x)$ is the Hessian matrix and $\\nabla f(x)$ is the gradient. If $H(x)$ has non-positive eigenvalues, we adjust it by adding $\\lambda I$ where $\\lambda$ is slightly larger than $|\\text{min eigenvalue}|$.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Compute the gradient $\\nabla f(x)$ and the Hessian $H(x)$ at the current point.\n",
    "2. Correct $H(x)$ to ensure it is positive definite.\n",
    "3. Compute the inverse of the corrected Hessian.\n",
    "4. Update the point $x$ using the formula above.\n",
    "5. Check for convergence. If the change in $x$ or in $f(x)$ is below a threshold, stop; otherwise, repeat.\n",
    "\n",
    "**Rationale:**\n",
    "This method is particularly effective for functions that are well-approximated by a quadratic form near the optimum, due to its use of second-order information. The eigenvalue correction ensures that the update direction is always a descent direction, preventing divergences.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- The convergence threshold and the size of the eigenvalue correction can significantly affect the performance and stability of the method. These need to be chosen based on the scale and sensitivity of the objective function.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Fast convergence near the optimum when the function is quadratic.\n",
    "- **Limitations:** High computational cost from calculating and inverting the Hessian; potential numerical instability without modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a7034a116ad710c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.676623Z",
     "start_time": "2024-05-19T15:55:35.813498Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newton_eigen_method(func, gradient, hessian, x0, known_solution, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Newton method with eigenvalue correction to ensure the Hessian is positive definite.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The objective function to minimize.\n",
    "        gradient (callable): The gradient of the function.\n",
    "        hessian (callable): The Hessian of the function.\n",
    "        x0 (np.array): Initial guess for the parameters.\n",
    "        known_solution (np.array): Known solution for calculating the distance.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Convergence tolerance.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The optimized parameters at convergence.\n",
    "        int: Total number of iterations performed.\n",
    "        list: Detailed log of the optimization process.\n",
    "        float: Norm of the final gradient.\n",
    "        float: Distance to the known solution.\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    overall_log = []\n",
    "    for k in range(max_iter):\n",
    "        grad = gradient(x)\n",
    "        hess = hessian(x)\n",
    "\n",
    "        # Eigenvalue correction if not positive definite\n",
    "        eigvals, _ = np.linalg.eig(hess)\n",
    "        min_eigval = np.min(eigvals)\n",
    "        if min_eigval <= 0:\n",
    "            hess += np.eye(len(x)) * (-min_eigval + 1e-6)\n",
    "\n",
    "        pk = -np.linalg.solve(hess, grad)\n",
    "        # Ensure your line_search function handles this and returns alpha and ls_log\n",
    "        alpha, ls_log = line_search(func, gradient, x, pk)  \n",
    "        \n",
    "        x_new = x + alpha * pk\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "\n",
    "        overall_log.append({\n",
    "            'iteration': k + 1,\n",
    "            'x': x_new.copy(),\n",
    "            'gradient_norm': np.linalg.norm(grad),\n",
    "            'function_value': func(x_new),\n",
    "            'distance_to_solution': np.linalg.norm(x_new - known_solution),\n",
    "            'line_search_log': ls_log\n",
    "        })\n",
    "        x = x_new\n",
    "\n",
    "    final_gradient_norm = np.linalg.norm(gradient(x))\n",
    "    distance_to_solution = np.linalg.norm(x - known_solution)\n",
    "\n",
    "    return x, k + 1, overall_log, final_gradient_norm, distance_to_solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb2df7df91fe6fe8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.687933Z",
     "start_time": "2024-05-19T15:55:35.821547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization details: {'iteration': 1, 'x': array([0., 0.]), 'gradient_norm': 100.4987562112089, 'function_value': 0.0, 'distance_to_solution': 0.0, 'line_search_log': [{'iteration': 1, 'alpha': 1.0, 'function_value': 0.0, 'target_value': 549.89}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNewtonEigenMethod(unittest.TestCase):\n",
    "    def test_newton_eigen_convergence(self):\n",
    "        # Define a quadratic function for testing\n",
    "        def func(x):\n",
    "            return 0.5 * (x[0]**2 + 10 * x[1]**2)\n",
    "\n",
    "        def gradient(x):\n",
    "            return np.array([x[0], 10 * x[1]])\n",
    "\n",
    "        def hessian(x):\n",
    "            return np.array([[1, 0], [0, 10]])\n",
    "\n",
    "        x0 = np.array([10.0, -10.0])\n",
    "        known_solution = np.array([0.0, 0.0])\n",
    "\n",
    "        optimized_x, num_iters, log, _, _ = newton_eigen_method(\n",
    "            func, gradient, hessian, x0, known_solution, max_iter=100, tol=1e-6\n",
    "        )\n",
    "\n",
    "        # Check if solution converges to the known solution\n",
    "        np.testing.assert_allclose(optimized_x, known_solution, atol=1e-6, rtol=1e-6)\n",
    "        self.assertTrue(len(log) > 0, \"No logging information provided.\")\n",
    "        print(\"Optimization details:\", log[-1])\n",
    "\n",
    "test_suite = unittest.TestSuite()\n",
    "loader = unittest.TestLoader()\n",
    "test_suite.addTest(loader.loadTestsFromTestCase(TestNewtonEigenMethod))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(test_suite)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0842d1a49ed85",
   "metadata": {},
   "source": [
    "### Optimization Algorithms: Conjugate Gradient Methods\n",
    "\n",
    "With the line search method defined, we can now incorporate it into the conjugate gradient algorithm, an efficient method for solving large-scale optimization problems. The conjugate gradient method uses line search to determine the optimal step size in each iteration, enhancing the algorithm's overall effectiveness.\n",
    "\n",
    "Conjugate gradient methods are iterative techniques that build a sequence of conjugate directions, along which the function is minimized. The general approach involves computing a search direction that is a linear combination of the steepest descent direction and the previous search direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55155bde02fb3ff5",
   "metadata": {},
   "source": [
    "### Linear Conjugate Gradient (LCG)\n",
    "\n",
    "**Description:**\n",
    "The Linear Conjugate Gradient Method is an algorithm for solving systems of linear equations with a positive-definite matrix, or for solving linear optimization problems. In optimization, it is used for minimizing quadratic functions in the form $f(x) = \\frac{1}{2} x^T A x - b^T x$, but it can be generalized for non-quadratic functions using appropriate line search techniques.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "The objective function optimized by LCG is typically a quadratic function:\n",
    "\n",
    "$$ f(x) = \\frac{1}{2} x^T A x - b^T x + c $$\n",
    "\n",
    "The update rules for the conjugate gradient method in the context of optimization are:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Choose an initial guess $x_0$.\n",
    "   - Set initial gradient $g_0 = \\nabla f(x_0)$ and initial direction $d_0 = -g_0$.\n",
    "\n",
    "2. **Iterative Update:**\n",
    "   - For each iteration $k$, compute:\n",
    "     - Step size $\\alpha_k = \\frac{g_k^T g_k}{d_k^T A d_k}$.\n",
    "     - Update position $x_{k+1} = x_k + \\alpha_k d_k$.\n",
    "     - Update gradient $g_{k+1} = \\nabla f(x_{k+1})$.\n",
    "     - Compute $\\beta_{k+1} = \\frac{g_{k+1}^T g_{k+1}}{g_k^T g_k}$.\n",
    "     - Update direction $d_{k+1} = -g_{k+1} + \\beta_{k+1} d_k$.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Compute initial gradient and set initial direction.\n",
    "2. For each iteration, perform a line search to determine the optimal step size.\n",
    "3. Update the position and gradient based on the line search result.\n",
    "4. Adjust the search direction using the computed $\\beta$ value.\n",
    "5. Check for convergence. If the norm of the gradient is below a threshold, terminate.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "LCG is particularly effective for high-dimensional optimization problems where the Hessian matrix is sparse and computing or storing it explicitly is impractical. It requires only gradient evaluations and is more memory efficient compared to methods like BFGS or Newton's method.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- The tolerance for convergence (`tol`) and maximum iterations (`max_iter`) are crucial for preventing excessive computations and ensuring timely termination of the algorithm.\n",
    "- The line search method must be robust to ensure that $\\alpha_k$ appropriately minimizes along the direction $d_k$.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Efficient for large sparse systems; only requires gradient information and uses minimal memory.\n",
    "- **Limitations:** Convergence can be slow near the optimum; performance heavily depends on the line search quality and may stagnate in non-quadratic landscapes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bf603d297b19b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.688104Z",
     "start_time": "2024-05-19T15:55:35.827609Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_conjugate_gradient(func, grad, x0, known_solution, max_iter=1000, tol=1e-6):\n",
    "    x = np.array(x0)\n",
    "    g = grad(x)\n",
    "    d = -g\n",
    "    overall_log = []\n",
    "    for k in range(max_iter):\n",
    "        alpha_k, _ = line_search(func, grad, x, d)  # Ensure you have a line_search function\n",
    "        if alpha_k is None:\n",
    "            break\n",
    "        x_new = x + alpha_k * d\n",
    "        g_new = grad(x_new)\n",
    "        beta_k = np.dot(g_new, g_new) / np.dot(g, g)\n",
    "        d = -g_new + beta_k * d\n",
    "        g = g_new\n",
    "        x = x_new\n",
    "        overall_log.append({\n",
    "            'iteration': k + 1,\n",
    "            'x': x.copy(),\n",
    "            'gradient_norm': np.linalg.norm(g),\n",
    "            'function_value': func(x),\n",
    "            'distance_to_solution': np.linalg.norm(x - known_solution)\n",
    "        })\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "    return x, k, overall_log, np.linalg.norm(g), np.linalg.norm(x - known_solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f705253b1839e614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.723834Z",
     "start_time": "2024-05-19T15:55:35.841232Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.025s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iteration': 1, 'x': array([1.8, 3.6]), 'gradient_norm': 3.577708763999664, 'function_value': 3.2000000000000006, 'distance_to_solution': 1.788854381999832}\n",
      "{'iteration': 2, 'x': array([1.48, 2.96]), 'gradient_norm': 2.146625258399799, 'function_value': 1.152000000000001, 'distance_to_solution': 1.0733126291998996}\n",
      "{'iteration': 3, 'x': array([0.609088, 1.218176]), 'gradient_norm': 1.7482116104407963, 'function_value': 0.7640609587200007, 'distance_to_solution': 0.8741058052203982}\n",
      "{'iteration': 4, 'x': array([0.67778844, 1.35557688]), 'gradient_norm': 1.4409739091754885, 'function_value': 0.5191014517311222, 'distance_to_solution': 0.7204869545877443}\n",
      "{'iteration': 5, 'x': array([1.29977669, 2.59955339]), 'gradient_norm': 1.3406421262802704, 'function_value': 0.44933032768932113, 'distance_to_solution': 0.6703210631401352}\n",
      "{'iteration': 6, 'x': array([1.29843261, 2.59686522]), 'gradient_norm': 1.3346312085886298, 'function_value': 0.4453101157346867, 'distance_to_solution': 0.6673156042943149}\n",
      "{'iteration': 7, 'x': array([0.76005506, 1.52011012]), 'gradient_norm': 1.0730663904365707, 'function_value': 0.28786786957114274, 'distance_to_solution': 0.5365331952182854}\n",
      "{'iteration': 8, 'x': array([0.85324414, 1.70648829]), 'gradient_norm': 0.6563121405462548, 'function_value': 0.10768640645710173, 'distance_to_solution': 0.3281560702731274}\n",
      "{'iteration': 9, 'x': array([1.11922558, 2.23845116]), 'gradient_norm': 0.5331929961421155, 'function_value': 0.07107369278375147, 'distance_to_solution': 0.26659649807105773}\n",
      "{'iteration': 10, 'x': array([1.09750206, 2.19500412]), 'gradient_norm': 0.4360424736502038, 'function_value': 0.04753325970674717, 'distance_to_solution': 0.2180212368251019}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestLinearConjugateGradient(unittest.TestCase):\n",
    "    def test_linear_conjugate_gradient_multi_step(self):\n",
    "        # Define a simple quadratic function and its gradient\n",
    "        def func(x):\n",
    "            return (x[0] - 1)**2 + (x[1] - 2)**2\n",
    "        \n",
    "        def grad(x):\n",
    "            return np.array([2*(x[0] - 1), 2*(x[1] - 2)])\n",
    "\n",
    "        x0 = np.array([0.0, 0.0])\n",
    "        known_solution = np.array([1, 2])\n",
    "        x_final, num_iters, log, final_grad_norm, distance = linear_conjugate_gradient(\n",
    "            func, grad, x0, known_solution, max_iter=100, tol=1e-6\n",
    "        )\n",
    "\n",
    "        # Assert final position is close to known solution\n",
    "        np.testing.assert_allclose(x_final, known_solution, atol=1e-4)\n",
    "        # Assert the number of iterations is reasonable\n",
    "        self.assertTrue(num_iters > 0 and num_iters < 100)\n",
    "        # Assert the final gradient norm is small, indicating convergence\n",
    "        self.assertLess(final_grad_norm, 1e-6)\n",
    "        # Check detailed logs have been created\n",
    "        self.assertTrue(len(log) > 0)\n",
    "        # Optional: print some logs for manual inspection\n",
    "        for entry in log[:10]:  # Print first 10 entries to check\n",
    "            print(entry)\n",
    "\n",
    "# Run the tests\n",
    "test_suite = unittest.TestSuite()\n",
    "loader = unittest.TestLoader()\n",
    "test_suite.addTest(loader.loadTestsFromTestCase(TestLinearConjugateGradient))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(test_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b6eb72874cfcc",
   "metadata": {},
   "source": [
    "#### Fletcher-Reeves Method (FR)\n",
    "\n",
    "**Description:**\n",
    "The Fletcher-Reeves (FR) method is a conjugate gradient technique used for nonlinear optimization. It iteratively minimizes multivariable functions by combining the concepts of steepest descent and conjugate directions, without requiring the computation of the Hessian matrix.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "The method is used to minimize a function $f(x)$, where $x$ is a vector in $\\mathbb{R}^n$. It updates the variables by iteratively moving towards a minimum, using gradients and a series of conjugate vectors.\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Start with an initial guess $x_0$.\n",
    "   - Compute the initial gradient $g_0 = \\nabla f(x_0)$.\n",
    "   - Set the initial direction $d_0 = -g_0$.\n",
    "\n",
    "2. **Iterative Update:**\n",
    "   - For each iteration $k$, perform the following steps:\n",
    "     - Determine the step size $\\alpha_k$ using a line search to minimize $f(x_k + \\alpha_k d_k)$.\n",
    "     - Update the position: $x_{k+1} = x_k + \\alpha_k d_k$.\n",
    "     - Compute the new gradient: $g_{k+1} = \\nabla f(x_{k+1})$.\n",
    "     - Update the direction using the Fletcher-Reeves formula: $\\beta_{k+1} = \\frac{\\|g_{k+1}\\|^2}{\\|g_k\\|^2}$.\n",
    "     - Set the new direction: $d_{k+1} = -g_{k+1} + \\beta_{k+1} d_k$.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Initialize gradient and direction based on the first evaluation of the function's gradient.\n",
    "2. Use a line search to find the optimal step size that minimizes the function along the current direction.\n",
    "3. Update the position and gradient.\n",
    "4. Calculate the parameter $\\beta$ that defines the next conjugate direction.\n",
    "5. Repeat the process until convergence is achieved, typically when the gradient's norm falls below a specified tolerance.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "The Fletcher-Reeves method is particularly useful for functions where the Hessian matrix is difficult to compute or where the dimensionality is so high that other methods become computationally infeasible. It is suitable for smooth nonlinear optimization problems and is favored for its robustness and relatively simple implementation.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- Proper selection of the line search method is crucial for the performance of the Fletcher-Reeves method.\n",
    "- The tolerance for the gradient's norm and the maximum number of iterations are key parameters that control the termination of the algorithm.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Does not require the computation of the Hessian matrix, reducing memory and computational overhead.\n",
    "- **Limitations:** The performance can be highly sensitive to the line search quality. Poor line search strategies can lead to suboptimal convergence rates or convergence to non-optimal points. Additionally, it may exhibit slow convergence rates near saddle points or minimizers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ed51cefe27b00",
   "metadata": {},
   "source": [
    "#### Polak-Ribiere Method (PR)\n",
    "\n",
    "**Description:**\n",
    "The Polak-Ribiere (PR) method is an optimization algorithm that enhances the basic conjugate gradient approach by modifying the formula used to calculate the conjugate direction. It is particularly effective for non-quadratic, nonlinear optimization problems.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "The PR method aims to minimize a function $f(x)$, where $x$ is a vector in $\\mathbb{R}^n$. The updates are calculated by:\n",
    "1. **Initialization:**\n",
    "   - Choose an initial guess $x_0$.\n",
    "   - Compute the initial gradient $g_0 = \\nabla f(x_0)$.\n",
    "   - Set the initial direction $d_0 = -g_0$.\n",
    "\n",
    "2. **Iterative Update:**\n",
    "   - For each iteration $k$, perform the following steps:\n",
    "     - Determine the step size $\\alpha_k$ through a line search to minimize $f(x_k + \\alpha_k d_k)$.\n",
    "     - Update the position: $x_{k+1} = x_k + \\alpha_k d_k$.\n",
    "     - Compute the new gradient: $g_{k+1} = \\nabla f(x_{k+1})$.\n",
    "     - Update the direction using the Polak-Ribiere formula: $\\beta_{k+1} = \\frac{(g_{k+1} - g_k)^T g_{k+1}}{\\|g_k\\|^2}$.\n",
    "     - Set the new direction: $d_{k+1} = -g_{k+1} + \\beta_{k+1} d_k$.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Initialize gradient and direction based on the first evaluation of the function’s gradient.\n",
    "2. Use a line search to find the optimal step size that minimizes the function along the current direction.\n",
    "3. Update the position and gradient.\n",
    "4. Calculate the parameter $\\beta$ that modifies the next conjugate direction.\n",
    "5. Repeat the process until convergence is achieved, typically when the gradient’s norm falls below a specified tolerance.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "The Polak-Ribiere method adjusts the conjugate direction more effectively than the Fletcher-Reeves method, especially in cases where the function landscape is complex and non-quadratic. This leads to potentially faster convergence and improved robustness against the pitfalls of simpler gradient-based methods.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- Selection of the line search method is critical for ensuring good performance of the PR method.\n",
    "- Tolerance levels for stopping criteria and the maximum number of iterations need careful adjustment based on the problem characteristics.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Often achieves faster convergence than the Fletcher-Reeves method, particularly on non-quadratic functions. Requires only first derivatives.\n",
    "- **Limitations:** Like other conjugate gradient methods, its performance heavily depends on an effective line search strategy. It can be less effective if the underlying function is not well-behaved or if the gradient calculations are inaccurate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd7dda049de0f36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.865731Z",
     "start_time": "2024-05-19T15:55:35.871490Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient_method(func, grad_func, x0, known_solution, method='FR', max_iter=10000, tol=1e-6, alpha_init=1.0, rho=0.9, c=1e-4, ls_max_iter=50):\n",
    "    \"\"\"\n",
    "    Implements the conjugate gradient optimization algorithm using either Fletcher-Reeves or Polak-Ribiere update rules, with backtracking line search.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The objective function to minimize.\n",
    "        grad_func (callable): The gradient of the objective function.\n",
    "        x0 (np.array): Initial guess for the parameters.\n",
    "        known_solution (np.array): The known solution or global minimum for the function, used for calculating distance.\n",
    "        method (str): 'FR' for Fletcher-Reeves or 'PR' for Polak-Ribiere.\n",
    "        max_iter (int): Maximum number of iterations before termination.\n",
    "        tol (float): Tolerance for convergence, based on the norm of the gradient.\n",
    "        alpha_init (float): Initial step size for the line search.\n",
    "        rho (float): Contraction factor in the line search, typically between 0.1 and 0.9.\n",
    "        c (float): The Armijo rule constant in the line search.\n",
    "        ls_max_iter (int): Maximum number of iterations allowed in the line search.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - Final parameter values (np.array)\n",
    "               - Number of iterations performed (int)\n",
    "               - Detailed log of the optimization process (list of dicts)\n",
    "               - Final gradient norm (float)\n",
    "               - Distance to the known solution (float)\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    g = grad_func(x)\n",
    "    d = -g\n",
    "    overall_log = []\n",
    "    for i in range(max_iter):\n",
    "        alpha, ls_log = line_search(func, grad_func, x, d, alpha_init, rho, c, ls_max_iter)\n",
    "        x_new = x + alpha * d\n",
    "        g_new = grad_func(x_new)\n",
    "        \n",
    "        beta = np.dot(g_new, g_new) / np.dot(g, g) if method == 'FR' else np.dot(g_new, g_new - g) / np.dot(g, g)\n",
    "        \n",
    "        d = -g_new + beta * d\n",
    "        if np.linalg.norm(g_new) < tol:\n",
    "            break\n",
    "        x = x_new\n",
    "        g = g_new\n",
    "        \n",
    "        overall_log.append({\n",
    "            'iteration': i + 1,\n",
    "            'x': x.copy(),\n",
    "            'gradient_norm': np.linalg.norm(g),\n",
    "            'alpha': alpha,\n",
    "            'beta': beta,\n",
    "            'function_value': func(x),\n",
    "            'line_search_log': ls_log\n",
    "        })\n",
    "\n",
    "    final_gradient_norm = np.linalg.norm(g_new)\n",
    "    distance_to_solution = np.linalg.norm(x - known_solution)\n",
    "\n",
    "    return x, i, overall_log, final_gradient_norm, distance_to_solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc18d4174985e1",
   "metadata": {},
   "source": [
    "### Optimization Algorithms: Quasi-Newton Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598cdd84a8593c",
   "metadata": {},
   "source": [
    "#### Broyden-Fletcher-Goldfarb-Shanno (BFGS) Method\n",
    "\n",
    "**Description:**\n",
    "The BFGS method is a quasi-Newton technique for finding local maxima and minima of functions. Unlike true Newton methods which require the computation of the Hessian matrix, BFGS uses an approximation to the Hessian matrix which is updated iteratively at each step of the optimization process.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "The BFGS method aims to solve the optimization problem for a function $f(x)$ where $x$ is a vector in $\\mathbb{R}^n$. The algorithm updates the approximation to the inverse Hessian matrix $H_k$ and the position vector $x_k$ iteratively as follows:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Start with an initial guess $x_0$.\n",
    "   - Initialize the inverse Hessian approximation, $H_0$, typically to the identity matrix.\n",
    "\n",
    "2. **Iterative Update:**\n",
    "   - For each iteration $k$, perform the following steps:\n",
    "     - Compute the gradient: $g_k = \\nabla f(x_k)$.\n",
    "     - Compute the search direction: $p_k = -H_k g_k$.\n",
    "     - Determine the step size $\\alpha_k$ using a line search to minimize $f(x_k + \\alpha_k p_k)$.\n",
    "     - Update the position: $x_{k+1} = x_k + \\alpha_k p_k$.\n",
    "     - Update the gradient: $g_{k+1} = \\nabla f(x_{k+1})$.\n",
    "     - Compute the difference vectors: $s_k = x_{k+1} - x_k$ and $y_k = g_{k+1} - g_k$.\n",
    "     - Update the inverse Hessian approximation using the BFGS formula:\n",
    "       $$ H_{k+1} = (I - \\rho_k s_k y_k^T) H_k (I - \\rho_k y_k s_k^T) + \\rho_k s_k s_k^T $$\n",
    "       where $\\rho_k = \\frac{1}{y_k^T s_k}$.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Calculate the gradient and determine the search direction.\n",
    "2. Use a line search to compute the step size.\n",
    "3. Update the position and gradient.\n",
    "4. Adjust the inverse Hessian approximation.\n",
    "5. Repeat the process until the norm of the gradient is less than a specified tolerance, indicating convergence.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "The BFGS method is used extensively in applications requiring optimization without explicit second derivatives. It is robust, reliable, and often performs well in practical situations.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- The initial approximation of the inverse Hessian ($H_0$) is crucial. While the identity matrix is a common choice, other problem-specific initializations can enhance performance.\n",
    "- The choice of line search method can significantly affect the convergence properties of the BFGS method.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Does not require the Hessian matrix, only first derivatives. Typically has superlinear convergence and is efficient for a wide range of problems.\n",
    "- **Limitations:** The memory requirement can be substantial for large-dimensional problems due to the need to store the inverse Hessian approximation. The performance can degrade if the line search is not performed adequately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf9e3907920cf383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.865869Z",
     "start_time": "2024-05-19T15:55:35.876419Z"
    }
   },
   "outputs": [],
   "source": [
    "def bfgs_update(Hk, sk, yk, quasi_zero=1e-12):\n",
    "    \"\"\" Perform the BFGS update on the approximate Hessian (or its inverse).\n",
    "    \n",
    "    Args:\n",
    "        Hk (np.ndarray): Current Hessian approximation.\n",
    "        sk (np.ndarray): Step vector x_k+1 - x_k.\n",
    "        yk (np.ndarray): Change in gradients grad f(x_k+1) - grad f(x_k).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Updated Hessian approximation.\n",
    "    \"\"\"\n",
    "    ykTsk = np.dot(yk.T, sk)\n",
    "    if np.abs(ykTsk) > quasi_zero:\n",
    "        rho_k = 1 / ykTsk\n",
    "        return Hk + rho_k * np.outer(sk, sk) - np.dot(Hk, np.outer(yk, yk)).dot(Hk) / np.dot(yk.T, Hk.dot(yk))\n",
    "    else:\n",
    "        return Hk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b9b31076b302845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.876378Z",
     "start_time": "2024-05-19T15:55:35.879010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "# Define the BFGS update function\n",
    "#def bfgs_update_H(H, s, y):\n",
    "#    rho = 1 / np.dot(y, s)\n",
    "#    I = np.eye(len(s))\n",
    "#    V = np.outer(s, y)\n",
    "#    H_next = (I - rho * V) @ H @ (I - rho * V.T) + rho * np.outer(s, s)\n",
    "#    return H_next\n",
    "def bfgs_update_B(Bk, sk, yk, quasi_zero=1e-12):\n",
    "    ykTsk = np.dot(yk.T, sk)\n",
    "    if np.abs(ykTsk) > quasi_zero:\n",
    "        rho_k = 1 / np.dot(yk.T, sk)\n",
    "        term1 = np.eye(len(Bk)) - rho_k * np.outer(yk, sk)\n",
    "        term2 = np.eye(len(Bk)) - rho_k * np.outer(sk, yk)\n",
    "        return np.dot(term1, np.dot(Bk, term2)) + rho_k * np.outer(yk, yk)\n",
    "    else:\n",
    "        return Bk\n",
    "\n",
    "def bfgs_update_H(Hk, sk, yk, quasi_zero=1e-12):\n",
    "    ykTsk = np.dot(yk.T, sk)\n",
    "    if ykTsk > quasi_zero:\n",
    "        rho_k = 1 / np.dot(yk.T, sk)\n",
    "        return Hk + rho_k * np.outer(sk, sk) - np.dot(Hk, np.outer(yk, yk)).dot(Hk) / np.dot(yk.T, Hk.dot(yk))\n",
    "    else:\n",
    "        return Hk\n",
    "\n",
    "# Create a test class\n",
    "class TestBFGSHessianAndInverseUpdate(unittest.TestCase):\n",
    "    def test_bfgs_Hk_update_multi_step(self):\n",
    "        # Define parameters for a simple quadratic function f(x) = 1/2 x^T Q x - b^T x\n",
    "        Q = np.array([[4, 1], [1, 3]])  # True Hessian\n",
    "        Q_inv = np.linalg.inv(Q)  # True inverse Hessian\n",
    "        b = np.array([1, 1])\n",
    "        x = np.array([1.0, 1.0])  # Initial point\n",
    "        grad_f = lambda x: np.dot(Q, x) - b\n",
    "        \n",
    "        H = np.eye(2)  # Start with identity matrix as initial inverse Hessian approximation\n",
    "        B = np.eye(2)  # Start with identity matrix as initial inverse Hessian approximation\n",
    "        for _ in range(10):  # Perform multiple BFGS updates\n",
    "            g_initial = grad_f(x)\n",
    "            s = -np.dot(H, g_initial)  # Step using the current inverse Hessian\n",
    "            x_new = x + s\n",
    "            g_new = grad_f(x_new)\n",
    "            y = g_new - g_initial\n",
    "            \n",
    "            B = bfgs_update_B(B, s, y)  # Update H_k using BFGS formula\n",
    "            H = bfgs_update_H(H, s, y)  # Update H_k using BFGS formula\n",
    "            x = x_new\n",
    "        \n",
    "        # Check if H has converged to the inverse of the true Hessian Q\n",
    "        np.testing.assert_allclose(B, Q, atol=1e-3, rtol=1e-3, err_msg=\"BFGS B_k did not converge to true Hessian after multiple steps\")\n",
    "        np.testing.assert_allclose(H, Q_inv, atol=1e-3, rtol=1e-3, err_msg=\"BFGS H_k did not converge to true inverse Hessian after multiple steps\")\n",
    "\n",
    "# Execute the test\n",
    "test_suite = unittest.TestSuite()\n",
    "loader = unittest.TestLoader()\n",
    "test_suite.addTest(loader.loadTestsFromTestCase(TestBFGSHessianAndInverseUpdate))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(test_suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb96694e81e12c0",
   "metadata": {},
   "source": [
    "#### Symmetric Rank 1 (SR1) Update Method\n",
    "\n",
    "**Description:**\n",
    "The SR1 method is a quasi-Newton technique used for optimization that updates the approximate Hessian matrix or its inverse using a rank-1 update. This method is particularly effective when the exact Hessian is indefinite or changes rapidly, conditions under which traditional quasi-Newton methods like BFGS might fail or perform poorly.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "SR1 updates the inverse of the Hessian matrix using a formula that does not enforce symmetry and positive definiteness, giving it flexibility in handling diverse types of objective functions:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Start with an initial guess \\( x_0 \\).\n",
    "   - Initialize the inverse Hessian approximation \\( H_0 \\), often as the identity matrix.\n",
    "\n",
    "2. **Iterative Update:**\n",
    "   - For each iteration \\( k \\), execute the following:\n",
    "     - Compute the gradient: \\( g_k = \\nabla f(x_k) \\).\n",
    "     - Compute the search direction: \\( p_k = -H_k g_k \\).\n",
    "     - Determine the step size \\( \\alpha_k \\) using a line search to minimize \\( f(x_k + \\alpha_k p_k) \\).\n",
    "     - Update the position: \\( x_{k+1} = x_k + \\alpha_k p_k \\).\n",
    "     - Update the gradient: \\( g_{k+1} = \\nabla f(x_{k+1}) \\).\n",
    "     - Calculate the change vectors: \\( s_k = x_{k+1} - x_k \\) and \\( y_k = g_{k+1} - g_k \\).\n",
    "     - If \\( (s_k - H_k y_k)^T y_k \\neq 0 \\), update the inverse Hessian approximation:\n",
    "       $$ H_{k+1} = H_k + \\frac{(s_k - H_k y_k)(s_k - H_k y_k)^T}{(s_k - H_k y_k)^T y_k} $$\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Evaluate the gradient and decide the search direction.\n",
    "2. Use line search to find the appropriate step size.\n",
    "3. Update the position and evaluate the new gradient.\n",
    "4. Adjust the inverse Hessian approximation using the SR1 update formula.\n",
    "5. Check for convergence based on the gradient norm or a change in function value, and repeat the steps if necessary.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "SR1 is beneficial in situations where the Hessian has significant curvature changes that are not well captured by other quasi-Newton updates. Its flexibility can lead to superior convergence properties in non-quadratic and ill-conditioned problems.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- The initial approximation \\( H_0 \\) and the line search method are crucial for the performance and stability of the SR1 method.\n",
    "- It's essential to handle cases where the denominator in the update formula approaches zero, which might require skipping the update.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Potentially more accurate Hessian approximation in dynamically changing landscapes. Less restrictive update rule can adapt better to complex objective functions.\n",
    "- **Limitations:** The SR1 update can fail if the curvature condition \\( (s_k - H_k y_k)^T y_k = 0 \\) is met, requiring fallback strategies. It may also converge slower than BFGS in some cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdc39378bf8886e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.881273Z",
     "start_time": "2024-05-19T15:55:35.896279Z"
    }
   },
   "outputs": [],
   "source": [
    "def sr1_update(H, s, y, quasi_zero=1e-12):\n",
    "    \"\"\" Perform the SR1 update on the approximate Hessian (or its inverse).\n",
    "    \n",
    "    Args:\n",
    "        H (np.ndarray): Current Hessian approximation.\n",
    "        s (np.ndarray): Step vector x_k+1 - x_k.\n",
    "        y (np.ndarray): Change in gradients grad f(x_k+1) - grad f(x_k).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Updated Hessian approximation if curvature condition is met.\n",
    "    \"\"\"\n",
    "    u = y - H @ s\n",
    "    uT_s = np.dot(u, s)\n",
    "    if np.abs(uT_s) > quasi_zero:  # avoiding division by zero or very small numbers\n",
    "        H += np.outer(u, u) / uT_s\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e34214fa2a5a440e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.883754Z",
     "start_time": "2024-05-19T15:55:35.906427Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import unittest\n",
    "\n",
    "def sr1_update(B, s, y):\n",
    "    \"\"\" Perform the SR1 update on the approximate Hessian B.\n",
    "    \n",
    "    Args:\n",
    "        B (np.ndarray): Current Hessian approximation.\n",
    "        s (np.ndarray): Step vector x_{k+1} - x_k.\n",
    "        y (np.ndarray): Change in gradients grad f(x_{k+1}) - grad f(x_k).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Updated Hessian approximation.\n",
    "    \"\"\"\n",
    "    Bs = np.dot(B, s)\n",
    "    u = y - Bs\n",
    "    uTs = np.dot(u.T, s)\n",
    "    if np.abs(uTs) > 1e-8:  # Avoid division by zero or very small numbers\n",
    "        B_next = B + np.outer(u, u) / uTs\n",
    "        return B_next\n",
    "    return B  # Skip the update if the denominator is too small\n",
    "\n",
    "class TestSR1MultiStepUpdate(unittest.TestCase):\n",
    "    def test_sr1_multi_step_update_quadratic(self):\n",
    "        # Define parameters for a simple quadratic function f(x) = 1/2 x^T Q x - b^T x\n",
    "        Q = np.array([[3, 0.5], [0.5, 2]])  # True Hessian\n",
    "        b = np.array([1, 1])\n",
    "        x = np.array([0.0, 0.0])  # Initial point\n",
    "        grad_f = lambda x: np.dot(Q, x) - b\n",
    "        \n",
    "        B = np.eye(2)  # Start with identity matrix as initial Hessian approximation\n",
    "        for _ in range(10):  # Perform multiple SR1 updates\n",
    "            g_initial = grad_f(x)\n",
    "            # Simplified step calculation: s = -B^-1 * g\n",
    "            s = -np.linalg.solve(B, g_initial)\n",
    "            x_new = x + s\n",
    "            g_new = grad_f(x_new)\n",
    "            y = g_new - g_initial\n",
    "            \n",
    "            B = sr1_update(B, s, y)  # Update B using SR1 formula\n",
    "            x = x_new\n",
    "        \n",
    "        # Check if B has converged to the true Hessian Q\n",
    "        np.testing.assert_allclose(B, Q, atol=1e-3, rtol=1e-3, err_msg=\"SR1 did not converge to true Hessian after multiple steps\")\n",
    "\n",
    "test_suite = unittest.TestSuite()\n",
    "loader = unittest.TestLoader()\n",
    "test_suite.addTest(loader.loadTestsFromTestCase(TestSR1MultiStepUpdate))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(test_suite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7495033500592f5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.883932Z",
     "start_time": "2024-05-19T15:55:35.914969Z"
    }
   },
   "outputs": [],
   "source": [
    "def quasi_newton_method(func, grad, x0, known_solution, method='BFGS', max_iter=1000, tol=1e-6, ls_max_iter=50):\n",
    "    \"\"\" Generic optimization using quasi-Newton methods BFGS or SR1.\n",
    "    \n",
    "    Args:\n",
    "        func (callable): The function to minimize.\n",
    "        grad (callable): The gradient of the function.\n",
    "        x0 (np.array): Initial guess for the solution.\n",
    "        known_solution (np.array): The known solution or global minimum for the function, used for calculating distance.\n",
    "        method (str): Specifies the update method ('BFGS' or 'SR1').\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Convergence tolerance.\n",
    "        ls_max_iter<. Msximum number of iterations in line search\n",
    "    \n",
    "    Returns:\n",
    "        np.array: The optimized variable values.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    n = len(x0)\n",
    "    H = np.eye(n)  # Start with the identity matrix\n",
    "    overall_log = []\n",
    "    for i in range(max_iter):\n",
    "        g = grad(x)\n",
    "        if np.linalg.norm(g) < tol:\n",
    "            break\n",
    "        p = -np.linalg.solve(H, g)\n",
    "        alpha, ls_log = line_search(func, grad, x, p, ls_max_iter)\n",
    "        #alpha = 0.5 # for now\n",
    "        s = alpha * p\n",
    "        x_new = x + s\n",
    "        y = grad(x_new) - g\n",
    "\n",
    "        # Update the Hessian approximation based on the method\n",
    "        if method == 'BFGS':\n",
    "            H = bfgs_update(H, s, y)\n",
    "        elif method == 'SR1':\n",
    "            H = sr1_update(H, s, y)\n",
    "        else:\n",
    "            raise ValueError('Invalid method: ', method)\n",
    "        \n",
    "        x = x_new\n",
    "        overall_log.append({\n",
    "            'iteration': i + 1,\n",
    "            'x': x.copy(),\n",
    "            'gradient_norm': np.linalg.norm(g),\n",
    "            'alpha': alpha,\n",
    "            'function_value': func(x),\n",
    "            'method': method\n",
    "        })\n",
    "        if np.linalg.norm(s) < tol:  # Additional stopping condition based on position change\n",
    "            break\n",
    "    \n",
    "    final_gradient_norm = np.linalg.norm(g)\n",
    "    distance_to_solution = np.linalg.norm(x - known_solution)\n",
    "    return x, i, overall_log, final_gradient_norm, distance_to_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7693ec36bc3f1c",
   "metadata": {},
   "source": [
    "### Gradient and Hessian Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976151ac1da4e6e1",
   "metadata": {},
   "source": [
    "#### Gradient Approximation Using Central Difference\n",
    "\n",
    "**Description:**\n",
    "Gradient approximation using the central difference formula is a numerical differentiation technique used to estimate the gradient of a scalar function. This method provides a more accurate approximation than the forward or backward difference methods by taking an average of slopes on both sides of a point.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "Given a scalar function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the gradient of $f$ at a point $x \\in \\mathbb{R}^n$ can be approximated using the central difference formula for each component $i$ of the gradient:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x_i}(x) \\approx \\frac{f(x + h e_i) - f(x - h e_i)}{2h} $$\n",
    "\n",
    "where:\n",
    "- $h$ is a small step size,\n",
    "- $e_i$ is the unit vector in the direction of the $i$-th coordinate.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Choose a small value for $h$, typically on the order of $10^{-5}$ to $10^{-8}$, depending on the function's sensitivity and floating-point precision considerations.\n",
    "2. For each dimension $i$ from 1 to $n$:\n",
    "   - Compute $f(x + h e_i)$ and $f(x - h e_i)$.\n",
    "   - Use the central difference formula to estimate the $i$-th component of the gradient.\n",
    "3. Combine the computed components to form the gradient vector at $x$.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "Central difference is used when the function's analytical gradient is unavailable or when the function is defined implicitly by a simulation or experiment. It is widely used in optimization algorithms, especially for machine learning models trained with gradient-based methods where analytical gradients are not readily available.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- The choice of $h$ is critical; too large a value can introduce significant approximation errors, while too small a value may lead to numerical instability due to floating-point precision errors.\n",
    "- It's often useful to experiment with different values of $h$ to find a balance between accuracy and computational stability.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Simple to implement and does not require knowledge of the function's derivative. More accurate than forward and backward difference methods.\n",
    "- **Limitations:** More computationally intensive than forward or backward differences since it requires two function evaluations per gradient component. Numerical errors can still be significant if $h$ is not appropriately chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93c9b726b10ac85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.884132Z",
     "start_time": "2024-05-19T15:55:35.922351Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def finite_difference_gradient(func, x, h=1e-5):\n",
    "    \"\"\"\n",
    "    Approximates the gradient of a function at a given point using the central difference formula.\n",
    "    \n",
    "    Args:\n",
    "        func (callable): The function for which the gradient is to be approximated.\n",
    "        x (np.array): The point at which the gradient is to be approximated.\n",
    "        h (float): The step size for the finite difference approximation.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The approximated gradient as a numpy array.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x_plus = np.array(x, dtype=float)\n",
    "        x_minus = np.array(x, dtype=float)\n",
    "        x_plus[i] += h  # Increment x[i] by h\n",
    "        x_minus[i] -= h  # Decrement x[i] by h\n",
    "        grad[i] = (func(x_plus) - func(x_minus)) / (2 * h)  # Central difference for derivative\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ff425b7c8c500",
   "metadata": {},
   "source": [
    "#### Hessian Approximation Using Finite Differences\n",
    "\n",
    "**Description:**\n",
    "Hessian approximation using finite differences is a numerical method to estimate the Hessian matrix, which is the second-order partial derivatives matrix of a scalar function. This approximation is crucial for methods that utilize curvature information to find minima or maxima of functions, especially in the absence of explicit second derivative formulas.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "Given a scalar function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the Hessian matrix $H$ at a point $x \\in \\mathbb{R}^n$ can be approximated using finite differences. Each element $H_{ij}$ of the Hessian matrix is approximated as:\n",
    "\n",
    "$$ H_{ij}(x) \\approx \\frac{f(x + h e_i + h e_j) - f(x + h e_i) - f(x + h e_j) + f(x)}{h^2} $$\n",
    "\n",
    "where:\n",
    "- $h$ is a small step size,\n",
    "- $e_i$ and $e_j$ are unit vectors in the directions of the $i$-th and $j$-th coordinates, respectively.\n",
    "\n",
    "**Algorithmic Steps:**\n",
    "1. Choose a small value for $h$, typically on the order of $10^{-5}$ to $10^{-8}$.\n",
    "2. For each pair of dimensions $(i, j)$:\n",
    "   - Compute the four function values needed for the finite difference approximation of the second derivative.\n",
    "   - Use the above formula to estimate each element $H_{ij}$ of the Hessian matrix.\n",
    "3. Assemble the computed elements into the full Hessian matrix.\n",
    "\n",
    "**Rationale and Use Case:**\n",
    "This method is particularly useful in optimization contexts where the second-order derivative (Hessian) information enhances the performance of algorithms, enabling more accurate and faster convergence to minima or maxima. It is essential for applications lacking analytical derivative expressions, such as complex engineering designs or machine learning models involving non-differentiable components.\n",
    "\n",
    "**Heuristics and Parameter Choices:**\n",
    "- The selection of $h$ is critical and should be chosen to balance the trade-off between numerical accuracy and stability. Too small a value may lead to floating-point precision issues, while too large a value can introduce significant approximation errors.\n",
    "- Symmetry of the Hessian can be enforced by averaging $H_{ij}$ and $H_{ji}$.\n",
    "\n",
    "**Advantages and Limitations:**\n",
    "- **Advantages:** Allows for the use of second-order methods when derivatives are not analytically available, improving convergence properties of optimization algorithms.\n",
    "- **Limitations:** Computationally expensive as it requires $O(n^2)$ function evaluations for a full Hessian matrix approximation. Numerical errors and instability can occur if $h$ is not appropriately chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bffc65cf6524bea8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.884347Z",
     "start_time": "2024-05-19T15:55:35.927812Z"
    }
   },
   "outputs": [],
   "source": [
    "def finite_difference_hessian(func, x, h=1e-4):\n",
    "    \"\"\"\n",
    "    Approximates the Hessian matrix of a function at a given point using finite differences.\n",
    "    \n",
    "    Args:\n",
    "        func (callable): The function for which the Hessian is to be approximated.\n",
    "        x (np.array): The point at which the Hessian is to be approximated.\n",
    "        h (float): The step size for the finite difference approximation.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The approximated Hessian matrix as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    hessian = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            x_ij = np.array(x, dtype=float)\n",
    "            if i == j:\n",
    "                # Diagonal entries\n",
    "                x_ij[i] += h\n",
    "                f_plus = func(x_ij)\n",
    "                x_ij[i] = x[i] - h\n",
    "                f_minus = func(x_ij)\n",
    "                hessian[i, j] = (f_plus - 2 * func(x) + f_minus) / h**2\n",
    "            else:\n",
    "                # Off-diagonal entries, use central difference for mixed partial derivatives\n",
    "                # f(x + h*ei + h*ej) - f(x + h*ei - h*ej) - f(x - h*ei + h*ej) + f(x - h*ei - h*ej)\n",
    "                x_plus_plus = np.array(x, dtype=float)\n",
    "                x_plus_minus = np.array(x, dtype=float)\n",
    "                x_minus_plus = np.array(x, dtype=float)\n",
    "                x_minus_minus = np.array(x, dtype=float)\n",
    "                \n",
    "                x_plus_plus[i] += h\n",
    "                x_plus_plus[j] += h\n",
    "                x_plus_minus[i] += h\n",
    "                x_plus_minus[j] -= h\n",
    "                x_minus_plus[i] -= h\n",
    "                x_minus_plus[j] += h\n",
    "                x_minus_minus[i] -= h\n",
    "                x_minus_minus[j] -= h\n",
    "                \n",
    "                f_plus_plus = func(x_plus_plus)\n",
    "                f_plus_minus = func(x_plus_minus)\n",
    "                f_minus_plus = func(x_minus_plus)\n",
    "                f_minus_minus = func(x_minus_minus)\n",
    "                \n",
    "                hessian[i, j] = (f_plus_plus - f_plus_minus - f_minus_plus + f_minus_minus) / (4 * h**2)\n",
    "    return hessian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f692f1d46987cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.884413Z",
     "start_time": "2024-05-19T15:55:35.934928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map each function to its corresponding gradient function\n",
    "grad_func_map = {\n",
    "    custom_function: grad_custom_function,\n",
    "    safe_rosenbrock: safe_grad_rosenbrock,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e0332c12324e12f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.884476Z",
     "start_time": "2024-05-19T15:55:35.938252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of defining a global map for Hessian functions\n",
    "hessian_func_map = {\n",
    "    custom_function: hessian_custom_function,  # Example: define `analytical_hessian_custom` appropriately\n",
    "    safe_rosenbrock: hessian_rosenbrock  # Define this function if needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "81985f81c85e7fb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.888798Z",
     "start_time": "2024-05-19T15:55:35.943256Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gradient_func(func, use_numerical_gradient=False):\n",
    "    \"\"\"\n",
    "    Retrieves the appropriate gradient function for a given objective function, allowing for the option\n",
    "    to use either the analytical or numerical gradient.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The objective function for which the gradient is required. This function\n",
    "                         should take a single numpy array as input and return a single value.\n",
    "        use_numerical_gradient (bool): A flag indicating whether to use a numerical gradient approximation.\n",
    "                                       If True, the gradient will be estimated using finite differences;\n",
    "                                       otherwise, the pre-defined analytical gradient will be used.\n",
    "\n",
    "    Returns:\n",
    "        callable: A function that computes the gradient of the objective function. This returned function\n",
    "                  takes a numpy array as input and returns a numpy array representing the gradient at that point.\n",
    "                  \n",
    "    Examples:\n",
    "        # Define an objective function\n",
    "        def my_function(x):\n",
    "            return x[0]**2 + x[1]**2 + 3*x[0]*x[1]\n",
    "\n",
    "        # Get the analytical gradient function\n",
    "        analytical_grad = get_gradient_func(my_function, use_numerical_gradient=False)\n",
    "        \n",
    "        # Get the numerical gradient function\n",
    "        numerical_grad = get_gradient_func(my_function, use_numerical_gradient=True)\n",
    "        \n",
    "        # Compute gradients\n",
    "        x = np.array([1.0, 2.0])\n",
    "        print(\"Analytical Gradient:\", analytical_grad(x))\n",
    "        print(\"Numerical Gradient:\", numerical_grad(x))\n",
    "    \"\"\"\n",
    "    if use_numerical_gradient:\n",
    "        # Use the numerical gradient approximation via finite differences\n",
    "        return lambda x: finite_difference_gradient(func, x)\n",
    "    else:\n",
    "        # Retrieve the analytical gradient from a predefined map\n",
    "        try:\n",
    "            return grad_func_map[func]\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Analytical gradient function not defined for the provided function.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac49fe63542919f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.888930Z",
     "start_time": "2024-05-19T15:55:35.947773Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_hessian_func(func, use_numerical_hessian=False, use_quasi_newton=False):\n",
    "    \"\"\"\n",
    "    Retrieves the appropriate Hessian function based on the configuration.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function for which the Hessian is needed.\n",
    "        use_numerical_hessian (bool): If True, use a numerical method to approximate the Hessian.\n",
    "        use_quasi_newton (bool): If True, use a quasi-Newton method for the Hessian approximation.\n",
    "\n",
    "    Returns:\n",
    "        callable: A function that computes the Hessian matrix for the given function.\n",
    "    \"\"\"\n",
    "    if use_numerical_hessian:\n",
    "        # Return a lambda function that calculates the numerical Hessian\n",
    "        return lambda x: finite_difference_hessian(func, x)\n",
    "    elif use_quasi_newton:\n",
    "        # If implementing a specific quasi-Newton method like BFGS that approximates the Hessian\n",
    "        # Normally BFGS would update its Hessian approximation internally, so this might be managed differently\n",
    "        return None  # Placeholder for quasi-Newton Hessian management\n",
    "    else:\n",
    "        # Return the analytical Hessian function mapped to `func`\n",
    "        return hessian_func_map[func]  # Ensure this map is defined somewhere globally\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb1e805510e0b4",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "We will test the optimization algorithms on two benchmark functions with the following initial points:\n",
    "\n",
    "- Rosenbrock function: Points `[1.2, 1.2]`, `[-1.2, 1.0]`, and `[0.2, 0.8]`.\n",
    "- Custom function: Points `[-0.2, 1.2]`, `[3.8, 0.1]`, and `[1.9, 0.6]`.\n",
    "\n",
    "Each test will be run with a convergence tolerance of \\(1e-6\\) and a maximum of 500000 iterations, allowing us to assess the efficiency and effectiveness of each method under various conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c284e2c960b84b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.889030Z",
     "start_time": "2024-05-19T15:55:35.951954Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define initial points for Rosenbrock function\n",
    "initial_points_rosenbrock = [np.array([1.2, 1.2]), np.array([-1.2, 1.0]), np.array([0.2, 0.8])]\n",
    "\n",
    "# Define initial points for the custom function\n",
    "initial_points_custom = [np.array([-0.2, 1.2]), np.array([3.8, 0.1]), np.array([1.9, 0.6])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdebb3be22e3d2e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T15:55:36.889317Z",
     "start_time": "2024-05-19T15:55:35.955078Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def run_optimization_test(opt_method, func, x0, known_solution, grad=None, hess=None, use_grad_approx=False, use_hess_approx=False, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Runs optimization tests using the specified optimization method, allowing for gradient and Hessian approximations.\n",
    "\n",
    "    Args:\n",
    "        opt_method (str): Specifies the optimization method variant ('FR', 'PR', 'BFGS', 'SR1', 'Newton', 'LCG').\n",
    "        func (callable): The objective function to minimize.\n",
    "        x0 (list of np.array): Initial points for the optimization.\n",
    "        known_solution (np.array): The known global minimum of the function.\n",
    "        grad (callable, optional): The exact gradient of the function, if available.\n",
    "        hess (callable, optional): The exact Hessian of the function, if applicable.\n",
    "        use_grad_approx (bool): Flag to use gradient approximation instead of exact gradient.\n",
    "        use_hess_approx (bool): Flag to use Hessian approximation instead of exact Hessian.\n",
    "        max_iter (int): Maximum number of iterations.\n",
    "        tol (float): Convergence tolerance.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing results from each optimization run.\n",
    "    \"\"\"\n",
    "    #print(f\"Processing opt_method={opt_method}, func={func}, x0={x0}, known_solution={known_solution}, grad={grad}, hess={hess}, use_grad_approx={use_grad_approx}, use_hess_approx={use_hess_approx}, max_iter={max_iter}, tol={tol}\")\n",
    "    results = []\n",
    "    current_grad = get_gradient_func(func, use_numerical_gradient=use_grad_approx)\n",
    "    current_hess = get_hessian_func(func, use_numerical_hessian=use_hess_approx) if opt_method in ['BFGS', 'SR1', 'Newton'] else None\n",
    "    #print(f\"Determined current_grad as {current_grad}, current_hess a {current_hess}\")\n",
    "    for initial_point in x0:\n",
    "        start = time.time()\n",
    "\n",
    "        if opt_method in ['FR', 'PR']:\n",
    "            x_final, num_iterations, overall_log, final_gradient_norm, distance_to_solution = conjugate_gradient_method(\n",
    "                func, current_grad, initial_point, known_solution, method=opt_method, max_iter=max_iter, tol=tol\n",
    "            )\n",
    "        elif opt_method in ['BFGS', 'SR1']:\n",
    "            x_final, num_iterations, overall_log, final_gradient_norm, distance_to_solution = quasi_newton_method(\n",
    "                func, current_grad, initial_point, known_solution, method=opt_method, max_iter=max_iter, tol=tol\n",
    "            )\n",
    "        elif opt_method in ['Newton']:\n",
    "            x_final, num_iterations, overall_log, final_gradient_norm, distance_to_solution = newton_eigen_method(\n",
    "                func, current_grad, current_hess, initial_point, known_solution, max_iter=max_iter, tol=tol\n",
    "            )\n",
    "        elif opt_method == 'LCG':\n",
    "            x_final, num_iterations, overall_log, final_gradient_norm, distance_to_solution = linear_conjugate_gradient(\n",
    "                func, current_grad, initial_point, known_solution, max_iter=max_iter, tol=tol\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f'Invalid optimization method: {opt_method}')\n",
    "\n",
    "        stop = time.time()\n",
    "        results.append({\n",
    "            'Starting Point': np.array_str(initial_point),\n",
    "            'Known Solution': np.array_str(known_solution),\n",
    "            'Calculated Solution': np.array_str(x_final),\n",
    "            'Distance to Solution': distance_to_solution,\n",
    "            'Final Gradient Norm': final_gradient_norm,\n",
    "            'Number of Iterations': num_iterations,\n",
    "            'Execution Time (s)': stop - start\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define finite difference methods if not already defined\n",
    "def finite_difference_gradient(func, x, epsilon=1e-8):\n",
    "    grad = np.zeros_like(x)\n",
    "    for i in range(len(x)):\n",
    "        x1 = np.array(x, dtype=float)\n",
    "        x2 = np.array(x, dtype=float)\n",
    "        x1[i] += epsilon\n",
    "        x2[i] -= epsilon\n",
    "        grad[i] = (func(x1) - func(x2)) / (2 * epsilon)\n",
    "    return grad\n",
    "\n",
    "def finite_difference_hessian(func, x, epsilon=1e-6):\n",
    "    n = len(x)\n",
    "    hessian = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            x_ijp = np.array(x, dtype=float)\n",
    "            x_ijm = np.array(x, dtype=float)\n",
    "            x_ijp[i] += epsilon\n",
    "            x_ijp[j] += epsilon\n",
    "            x_ijm[i] -= epsilon\n",
    "            x_ijm[j] -= epsilon\n",
    "            if i == j:\n",
    "                hessian[i, j] = (func(x_ijp) - 2 * func(x) + func(x_ijm)) / epsilon**2\n",
    "            else:\n",
    "                f_ip_jp = func(x_ijp)\n",
    "                x_ijp[j] -= 2 * epsilon\n",
    "                f_ip_jm = func(x_ijp)\n",
    "                x_ijm[i] += 2 * epsilon\n",
    "                f_im_jp = func(x_ijm)\n",
    "                x_ijm[j] += 2 * epsilon\n",
    "                f_im_jm = func(x_ijm)\n",
    "                hessian[i, j] = (f_ip_jp - f_ip_jm - f_im_jp + f_im_jm) / (4 * epsilon**2)\n",
    "    return hessian\n",
    "\n",
    "# Example usage\n",
    "#initial_points = [np.array([-1.2, 1.0]), np.array([1.0, 1.0]), np.array([-1.5, 1.5])]\n",
    "#initial_points = [np.array([-1.2, 1.0])]\n",
    "#known_solution = np.array([1, 1])\n",
    "\n",
    "#results_df = run_optimization_test(\n",
    "#    opt_method='BFGS',\n",
    "#    func=safe_rosenbrock,\n",
    "#    x0=initial_points,\n",
    "#    known_solution=known_solution,\n",
    "#    use_grad_approx=False,\n",
    "#    use_hess_approx=False\n",
    "#)\n",
    "#print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765ea6e3a653f9a",
   "metadata": {},
   "source": [
    "## Optimization Results\n",
    "\n",
    "Below are the detailed results from each optimization run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f91b150b05387b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T16:05:35.547181Z",
     "start_time": "2024-05-19T15:55:35.966915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (Newton):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]                  [1. 1.]  \\\n1    [-1.2  1. ]        [1. 1.]  [0.99999999 0.99999999]   \n2      [0.2 0.8]        [1. 1.]  [1.00000001 1.00000001]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          1.339488e-10         2.056839e-10                     8  \\\n1          1.290956e-08         7.557077e-09                    20   \n2          1.179722e-08         1.877807e-08                     9   \n\n   Execution Time (s)  \n0            0.002193  \n1            0.005942  \n2            0.009507  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[1. 1.]</td>\n      <td>1.339488e-10</td>\n      <td>2.056839e-10</td>\n      <td>8</td>\n      <td>0.002193</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999999 0.99999999]</td>\n      <td>1.290956e-08</td>\n      <td>7.557077e-09</td>\n      <td>20</td>\n      <td>0.005942</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[1.00000001 1.00000001]</td>\n      <td>1.179722e-08</td>\n      <td>1.877807e-08</td>\n      <td>9</td>\n      <td>0.009507</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (Newton):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution                Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [-1.86734201e-10  1.00000000e+00]  \\\n1      [3.8 0.1]        [4. 0.]  [ 4.00000000e+00 -2.46043639e-11]   \n2      [1.9 0.6]        [4. 0.]    [ 4.0000000e+00 -5.3447754e-13]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          4.123106e+00         5.476743e-08                     8  \\\n1          1.276079e-09         1.157476e-07                     8   \n2          2.035943e-10         2.164925e-09                    11   \n\n   Execution Time (s)  \n0            0.002190  \n1            0.003394  \n2            0.008478  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[-1.86734201e-10  1.00000000e+00]</td>\n      <td>4.123106e+00</td>\n      <td>5.476743e-08</td>\n      <td>8</td>\n      <td>0.002190</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[ 4.00000000e+00 -2.46043639e-11]</td>\n      <td>1.276079e-09</td>\n      <td>1.157476e-07</td>\n      <td>8</td>\n      <td>0.003394</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[ 4.0000000e+00 -5.3447754e-13]</td>\n      <td>2.035943e-10</td>\n      <td>2.164925e-09</td>\n      <td>11</td>\n      <td>0.008478</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (Linear Conjugate Gradient):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]  [0.99999999 0.99999999]  \\\n1    [-1.2  1. ]        [1. 1.]  [0.99999998 0.99999996]   \n2      [0.2 0.8]        [1. 1.]  [1.0000009  1.00000181]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          1.722343e-08         9.293173e-07                   358  \\\n1          4.056625e-08         9.637434e-07                   430   \n2          2.020182e-06         8.450254e-07                  1050   \n\n   Execution Time (s)  \n0            0.877453  \n1            1.106468  \n2            6.143862  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999999 0.99999999]</td>\n      <td>1.722343e-08</td>\n      <td>9.293173e-07</td>\n      <td>358</td>\n      <td>0.877453</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999998 0.99999996]</td>\n      <td>4.056625e-08</td>\n      <td>9.637434e-07</td>\n      <td>430</td>\n      <td>1.106468</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[1.0000009  1.00000181]</td>\n      <td>2.020182e-06</td>\n      <td>8.450254e-07</td>\n      <td>1050</td>\n      <td>6.143862</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (Linear Conjugate Gradient):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution                Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [-1.45078550e-09  9.99999923e-01]  \\\n1      [3.8 0.1]        [4. 0.]  [ 3.99999998e+00 -1.91559901e-10]   \n2      [1.9 0.6]        [4. 0.]  [ 4.00000003e+00 -1.95335249e-10]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          4.123106e+00         8.522154e-07                   671  \\\n1          1.865007e-08         9.583673e-07                   417   \n2          3.473588e-08         8.698669e-07                   426   \n\n   Execution Time (s)  \n0            0.317177  \n1            0.244025  \n2            0.249076  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[-1.45078550e-09  9.99999923e-01]</td>\n      <td>4.123106e+00</td>\n      <td>8.522154e-07</td>\n      <td>671</td>\n      <td>0.317177</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[ 3.99999998e+00 -1.91559901e-10]</td>\n      <td>1.865007e-08</td>\n      <td>9.583673e-07</td>\n      <td>417</td>\n      <td>0.244025</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[ 4.00000003e+00 -1.95335249e-10]</td>\n      <td>3.473588e-08</td>\n      <td>8.698669e-07</td>\n      <td>426</td>\n      <td>0.249076</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (Fletcher-Reeves):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]  [0.99999999 0.99999998]  \\\n1    [-1.2  1. ]        [1. 1.]  [0.99999998 0.99999996]   \n2      [0.2 0.8]        [1. 1.]    [1.0000009 1.0000018]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          1.826919e-08         9.293173e-07                   358  \\\n1          3.922507e-08         9.637434e-07                   430   \n2          2.017737e-06         8.450254e-07                  1050   \n\n   Execution Time (s)  \n0            1.054324  \n1            1.626041  \n2            5.172769  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999999 0.99999998]</td>\n      <td>1.826919e-08</td>\n      <td>9.293173e-07</td>\n      <td>358</td>\n      <td>1.054324</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999998 0.99999996]</td>\n      <td>3.922507e-08</td>\n      <td>9.637434e-07</td>\n      <td>430</td>\n      <td>1.626041</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[1.0000009 1.0000018]</td>\n      <td>2.017737e-06</td>\n      <td>8.450254e-07</td>\n      <td>1050</td>\n      <td>5.172769</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (Fletcher-Reeves):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution              Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [8.31564295e-09 9.99999937e-01]  \\\n1      [3.8 0.1]        [4. 0.]  [3.99999998e+00 2.33822998e-10]   \n2      [1.9 0.6]        [4. 0.]  [4.00000003e+00 1.97289911e-10]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          4.123106e+00         8.522154e-07                   671  \\\n1          1.880843e-08         9.583673e-07                   417   \n2          3.473033e-08         8.698669e-07                   426   \n\n   Execution Time (s)  \n0            0.290882  \n1            0.246925  \n2            0.628772  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[8.31564295e-09 9.99999937e-01]</td>\n      <td>4.123106e+00</td>\n      <td>8.522154e-07</td>\n      <td>671</td>\n      <td>0.290882</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[3.99999998e+00 2.33822998e-10]</td>\n      <td>1.880843e-08</td>\n      <td>9.583673e-07</td>\n      <td>417</td>\n      <td>0.246925</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[4.00000003e+00 1.97289911e-10]</td>\n      <td>3.473033e-08</td>\n      <td>8.698669e-07</td>\n      <td>426</td>\n      <td>0.628772</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (Polak-Ribiere):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]  [1.00276253 1.00546767]  \\\n1    [-1.2  1. ]        [1. 1.]  [1.03845515 1.07883445]   \n2      [0.2 0.8]        [1. 1.]    [1.461088  2.1398892]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0              0.006126             0.034174                  9999  \\\n1              0.087714             0.140053                  9999   \n2              1.229614             2.304076                  9999   \n\n   Execution Time (s)  \n0           85.208515  \n1           91.879519  \n2           90.890159  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[1.00276253 1.00546767]</td>\n      <td>0.006126</td>\n      <td>0.034174</td>\n      <td>9999</td>\n      <td>85.208515</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[1.03845515 1.07883445]</td>\n      <td>0.087714</td>\n      <td>0.140053</td>\n      <td>9999</td>\n      <td>91.879519</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[1.461088  2.1398892]</td>\n      <td>1.229614</td>\n      <td>2.304076</td>\n      <td>9999</td>\n      <td>90.890159</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (Polak-Ribiere):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution                Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [ 3.64593387e+00 -1.58040710e-04]  \\\n1      [3.8 0.1]        [4. 0.]    [3.91610695e+00 7.50409666e-05]   \n2      [1.9 0.6]        [4. 0.]  [-1.66884517e+01  2.91287428e-03]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0              0.354066             1.351324                  9999  \\\n1              0.083893             0.182899                  9999   \n2             20.688452           202.287445                  9999   \n\n   Execution Time (s)  \n0           20.950035  \n1           17.945809  \n2           19.004779  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[ 3.64593387e+00 -1.58040710e-04]</td>\n      <td>0.354066</td>\n      <td>1.351324</td>\n      <td>9999</td>\n      <td>20.950035</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[3.91610695e+00 7.50409666e-05]</td>\n      <td>0.083893</td>\n      <td>0.182899</td>\n      <td>9999</td>\n      <td>17.945809</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[-1.66884517e+01  2.91287428e-03]</td>\n      <td>20.688452</td>\n      <td>202.287445</td>\n      <td>9999</td>\n      <td>19.004779</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (BFGS):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]  [1.11119841 1.23515821]  \\\n1    [-1.2  1. ]        [1. 1.]  [1.42495122 2.03124789]   \n2      [0.2 0.8]        [1. 1.]  [1.82763086 3.34307014]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0              0.260124             0.091774                  9999  \\\n1              1.115372             0.437079                  9999   \n2              2.484945             0.641518                  9999   \n\n   Execution Time (s)  \n0           72.247314  \n1           74.640278  \n2           76.502512  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[1.11119841 1.23515821]</td>\n      <td>0.260124</td>\n      <td>0.091774</td>\n      <td>9999</td>\n      <td>72.247314</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[1.42495122 2.03124789]</td>\n      <td>1.115372</td>\n      <td>0.437079</td>\n      <td>9999</td>\n      <td>74.640278</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[1.82763086 3.34307014]</td>\n      <td>2.484945</td>\n      <td>0.641518</td>\n      <td>9999</td>\n      <td>76.502512</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (BFGS):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution                Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [-1.33701476e-05  1.00074215e+00]  \\\n1      [3.8 0.1]        [4. 0.]    [3.79773013e+00 9.50859422e-05]   \n2      [1.9 0.6]        [4. 0.]    [4.00036318e+00 2.01133823e-07]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0              4.123299             0.006464                  9999  \\\n1              0.202270             0.101289                  9999   \n2              0.000363             0.001714                  1325   \n\n   Execution Time (s)  \n0           13.593201  \n1           15.458590  \n2            2.424134  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[-1.33701476e-05  1.00074215e+00]</td>\n      <td>4.123299</td>\n      <td>0.006464</td>\n      <td>9999</td>\n      <td>13.593201</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[3.79773013e+00 9.50859422e-05]</td>\n      <td>0.202270</td>\n      <td>0.101289</td>\n      <td>9999</td>\n      <td>15.458590</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[4.00036318e+00 2.01133823e-07]</td>\n      <td>0.000363</td>\n      <td>0.001714</td>\n      <td>1325</td>\n      <td>2.424134</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (SR1):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]  [1.11119841 1.23515821]  \\\n1    [-1.2  1. ]        [1. 1.]  [1.42495122 2.03124789]   \n2      [0.2 0.8]        [1. 1.]  [1.82763086 3.34307014]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0              0.260124             0.091774                  9999  \\\n1              1.115372             0.437079                  9999   \n2              2.484945             0.641518                  9999   \n\n   Execution Time (s)  \n0           72.247314  \n1           74.640278  \n2           76.502512  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[1.11119841 1.23515821]</td>\n      <td>0.260124</td>\n      <td>0.091774</td>\n      <td>9999</td>\n      <td>72.247314</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[1.42495122 2.03124789]</td>\n      <td>1.115372</td>\n      <td>0.437079</td>\n      <td>9999</td>\n      <td>74.640278</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[1.82763086 3.34307014]</td>\n      <td>2.484945</td>\n      <td>0.641518</td>\n      <td>9999</td>\n      <td>76.502512</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (SR1):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution                Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [-1.33701476e-05  1.00074215e+00]  \\\n1      [3.8 0.1]        [4. 0.]    [3.79773013e+00 9.50859422e-05]   \n2      [1.9 0.6]        [4. 0.]    [4.00036318e+00 2.01133823e-07]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0              4.123299             0.006464                  9999  \\\n1              0.202270             0.101289                  9999   \n2              0.000363             0.001714                  1325   \n\n   Execution Time (s)  \n0           13.593201  \n1           15.458590  \n2            2.424134  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[-1.33701476e-05  1.00074215e+00]</td>\n      <td>4.123299</td>\n      <td>0.006464</td>\n      <td>9999</td>\n      <td>13.593201</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[3.79773013e+00 9.50859422e-05]</td>\n      <td>0.202270</td>\n      <td>0.101289</td>\n      <td>9999</td>\n      <td>15.458590</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[4.00036318e+00 2.01133823e-07]</td>\n      <td>0.000363</td>\n      <td>0.001714</td>\n      <td>1325</td>\n      <td>2.424134</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "known_solution_rosenbrock = np.array([1., 1.])\n",
    "known_solution_custom = np.array([4., 0.]) # or (0,1) //FIXME\n",
    "\n",
    "# Run tests and display results\n",
    "\n",
    "results_df_rosenbrock_nm = run_optimization_test(\n",
    "    opt_method='Newton',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Newton):\")\n",
    "display(results_df_rosenbrock_nm)\n",
    "\n",
    "results_df_custom_nm = run_optimization_test(\n",
    "    opt_method='Newton',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Newton):\")\n",
    "display(results_df_custom_nm)\n",
    "\n",
    "results_df_rosenbrock_lcg = run_optimization_test(\n",
    "    opt_method='LCG',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Linear Conjugate Gradient):\")\n",
    "display(results_df_rosenbrock_lcg)\n",
    "\n",
    "results_df_custom_lcg = run_optimization_test(\n",
    "    opt_method='LCG',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Linear Conjugate Gradient):\")\n",
    "display(results_df_custom_lcg)\n",
    "\n",
    "results_df_rosenbrock_fr = run_optimization_test(\n",
    "    opt_method='FR',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Fletcher-Reeves):\")\n",
    "display(results_df_rosenbrock_fr)\n",
    "\n",
    "results_df_custom_fr = run_optimization_test(\n",
    "    opt_method='FR',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Fletcher-Reeves):\")\n",
    "display(results_df_custom_fr)\n",
    "\n",
    "# ATTENTION! Running _long_: 4:50 for 10000 iters\n",
    "results_df_rosenbrock_pr = run_optimization_test(\n",
    "    opt_method='PR',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "   use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Polak-Ribiere):\")\n",
    "display(results_df_rosenbrock_pr)\n",
    "\n",
    "# ATTENTION! Running _long_\n",
    "results_df_custom_pr = run_optimization_test(\n",
    "    opt_method='PR',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Polak-Ribiere):\")\n",
    "display(results_df_custom_pr)\n",
    "\n",
    "# ATTENTION! Running _long_: 4:50 for 10000 iters\n",
    "results_df_rosenbrock_bfgs = run_optimization_test(\n",
    "    opt_method='BFGS',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (BFGS):\")\n",
    "display(results_df_rosenbrock_bfgs)\n",
    "\n",
    "# ATTENTION! Running _long_\n",
    "results_df_custom_bfgs = run_optimization_test(\n",
    "    opt_method='BFGS',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (BFGS):\")\n",
    "display(results_df_custom_bfgs)\n",
    "\n",
    "results_df_rosenbrock_sr1 = run_optimization_test(\n",
    "    opt_method='SR1',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (SR1):\")\n",
    "display(results_df_rosenbrock_bfgs)\n",
    "\n",
    "results_df_custom_sr1 = run_optimization_test(\n",
    "    opt_method='SR1',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=False,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (SR1):\")\n",
    "display(results_df_custom_bfgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2f6d8ddd7e4e5",
   "metadata": {},
   "source": [
    "With gradient and Hessian approximation (if applicable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d9b23cc5f963b",
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-19T16:05:35.509871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (Newton):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]                  [1. 1.]  \\\n1    [-1.2  1. ]        [1. 1.]  [0.99999999 0.99999999]   \n2      [0.2 0.8]        [1. 1.]  [1.00000001 1.00000001]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          1.344800e-10         2.078589e-10                     8  \\\n1          1.291168e-08         7.573559e-09                    20   \n2          1.180466e-08         1.880027e-08                     9   \n\n   Execution Time (s)  \n0            0.004013  \n1            0.011561  \n2            0.019549  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[1. 1.]</td>\n      <td>1.344800e-10</td>\n      <td>2.078589e-10</td>\n      <td>8</td>\n      <td>0.004013</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999999 0.99999999]</td>\n      <td>1.291168e-08</td>\n      <td>7.573559e-09</td>\n      <td>20</td>\n      <td>0.011561</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[1.00000001 1.00000001]</td>\n      <td>1.180466e-08</td>\n      <td>1.880027e-08</td>\n      <td>9</td>\n      <td>0.019549</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (Newton):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution                Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [-1.86735967e-10  1.00000000e+00]  \\\n1      [3.8 0.1]        [4. 0.]  [ 4.00000000e+00 -2.46022985e-11]   \n2      [1.9 0.6]        [4. 0.]  [ 4.00000000e+00 -4.13355549e-12]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          4.123106e+00         5.476784e-08                     8  \\\n1          1.276370e-09         1.157371e-07                     8   \n2          1.549486e-09         1.679268e-08                    11   \n\n   Execution Time (s)  \n0            0.007184  \n1            0.007957  \n2            0.015792  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[-1.86735967e-10  1.00000000e+00]</td>\n      <td>4.123106e+00</td>\n      <td>5.476784e-08</td>\n      <td>8</td>\n      <td>0.007184</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[ 4.00000000e+00 -2.46022985e-11]</td>\n      <td>1.276370e-09</td>\n      <td>1.157371e-07</td>\n      <td>8</td>\n      <td>0.007957</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[ 4.00000000e+00 -4.13355549e-12]</td>\n      <td>1.549486e-09</td>\n      <td>1.679268e-08</td>\n      <td>11</td>\n      <td>0.015792</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (Linear Conjugate Gradient):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]  [1.00000005 1.0000001 ]  \\\n1    [-1.2  1. ]        [1. 1.]  [1.00000105 1.0000021 ]   \n2      [0.2 0.8]        [1. 1.]  [0.99999904 0.99999808]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          1.139591e-07         9.512816e-07                   296  \\\n1          2.344252e-06         9.366804e-07                   650   \n2          2.145234e-06         8.684976e-07                   536   \n\n   Execution Time (s)  \n0            2.089635  \n1            4.204779  \n2            3.742416  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[1.00000005 1.0000001 ]</td>\n      <td>1.139591e-07</td>\n      <td>9.512816e-07</td>\n      <td>296</td>\n      <td>2.089635</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[1.00000105 1.0000021 ]</td>\n      <td>2.344252e-06</td>\n      <td>9.366804e-07</td>\n      <td>650</td>\n      <td>4.204779</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999904 0.99999808]</td>\n      <td>2.145234e-06</td>\n      <td>8.684976e-07</td>\n      <td>536</td>\n      <td>3.742416</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (Linear Conjugate Gradient):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution                Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]    [8.25406356e-10 9.99999879e-01]  \\\n1      [3.8 0.1]        [4. 0.]  [ 3.99999994e+00 -1.79507442e-10]   \n2      [1.9 0.6]        [4. 0.]  [ 4.00000009e+00 -2.26491052e-10]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          4.123106e+00         9.680868e-07                   653  \\\n1          5.596716e-08         9.754174e-07                   469   \n2          8.738977e-08         9.152122e-07                   452   \n\n   Execution Time (s)  \n0            0.950621  \n1            1.212040  \n2            0.788856  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[8.25406356e-10 9.99999879e-01]</td>\n      <td>4.123106e+00</td>\n      <td>9.680868e-07</td>\n      <td>653</td>\n      <td>0.950621</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[ 3.99999994e+00 -1.79507442e-10]</td>\n      <td>5.596716e-08</td>\n      <td>9.754174e-07</td>\n      <td>469</td>\n      <td>1.212040</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[ 4.00000009e+00 -2.26491052e-10]</td>\n      <td>8.738977e-08</td>\n      <td>9.152122e-07</td>\n      <td>452</td>\n      <td>0.788856</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Rosenbrock Function (Fletcher-Reeves):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution      Calculated Solution   \n0      [1.2 1.2]        [1. 1.]  [1.00000005 1.0000001 ]  \\\n1    [-1.2  1. ]        [1. 1.]  [1.00000105 1.00000209]   \n2      [0.2 0.8]        [1. 1.]  [0.99999904 0.99999808]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          1.140972e-07         9.512816e-07                   296  \\\n1          2.338675e-06         9.366804e-07                   650   \n2          2.143373e-06         8.684976e-07                   536   \n\n   Execution Time (s)  \n0            2.757622  \n1            4.775309  \n2            4.039246  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[1.2 1.2]</td>\n      <td>[1. 1.]</td>\n      <td>[1.00000005 1.0000001 ]</td>\n      <td>1.140972e-07</td>\n      <td>9.512816e-07</td>\n      <td>296</td>\n      <td>2.757622</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[-1.2  1. ]</td>\n      <td>[1. 1.]</td>\n      <td>[1.00000105 1.00000209]</td>\n      <td>2.338675e-06</td>\n      <td>9.366804e-07</td>\n      <td>650</td>\n      <td>4.775309</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[0.2 0.8]</td>\n      <td>[1. 1.]</td>\n      <td>[0.99999904 0.99999808]</td>\n      <td>2.143373e-06</td>\n      <td>8.684976e-07</td>\n      <td>536</td>\n      <td>4.039246</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Results for Custom Function (Fletcher-Reeves):'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  Starting Point Known Solution              Calculated Solution   \n0    [-0.2  1.2]        [4. 0.]  [1.88043448e-08 9.99999942e-01]  \\\n1      [3.8 0.1]        [4. 0.]  [3.99999994e+00 2.32950144e-10]   \n2      [1.9 0.6]        [4. 0.]  [4.00000009e+00 1.83707111e-10]   \n\n   Distance to Solution  Final Gradient Norm  Number of Iterations   \n0          4.123106e+00         9.680868e-07                   653  \\\n1          5.584727e-08         9.754174e-07                   469   \n2          8.753636e-08         9.152122e-07                   452   \n\n   Execution Time (s)  \n0            1.301962  \n1            0.897800  \n2            0.769504  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Starting Point</th>\n      <th>Known Solution</th>\n      <th>Calculated Solution</th>\n      <th>Distance to Solution</th>\n      <th>Final Gradient Norm</th>\n      <th>Number of Iterations</th>\n      <th>Execution Time (s)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[-0.2  1.2]</td>\n      <td>[4. 0.]</td>\n      <td>[1.88043448e-08 9.99999942e-01]</td>\n      <td>4.123106e+00</td>\n      <td>9.680868e-07</td>\n      <td>653</td>\n      <td>1.301962</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[3.8 0.1]</td>\n      <td>[4. 0.]</td>\n      <td>[3.99999994e+00 2.32950144e-10]</td>\n      <td>5.584727e-08</td>\n      <td>9.754174e-07</td>\n      <td>469</td>\n      <td>0.897800</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[1.9 0.6]</td>\n      <td>[4. 0.]</td>\n      <td>[4.00000009e+00 1.83707111e-10]</td>\n      <td>8.753636e-08</td>\n      <td>9.152122e-07</td>\n      <td>452</td>\n      <td>0.769504</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "known_solution_rosenbrock = np.array([1., 1.])\n",
    "known_solution_custom = np.array([4., 0.]) # or (0,1) //FIXME\n",
    "\n",
    "# Run tests and display results\n",
    "\n",
    "results_df_rosenbrock_nm = run_optimization_test(\n",
    "    opt_method='Newton',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Newton):\")\n",
    "display(results_df_rosenbrock_nm)\n",
    "\n",
    "results_df_custom_nm = run_optimization_test(\n",
    "    opt_method='Newton',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Newton):\")\n",
    "display(results_df_custom_nm)\n",
    "\n",
    "results_df_rosenbrock_lcg = run_optimization_test(\n",
    "    opt_method='LCG',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Linear Conjugate Gradient):\")\n",
    "display(results_df_rosenbrock_lcg)\n",
    "\n",
    "results_df_custom_lcg = run_optimization_test(\n",
    "    opt_method='LCG',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Linear Conjugate Gradient):\")\n",
    "display(results_df_custom_lcg)\n",
    "\n",
    "results_df_rosenbrock_fr = run_optimization_test(\n",
    "    opt_method='FR',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Fletcher-Reeves):\")\n",
    "display(results_df_rosenbrock_fr)\n",
    "\n",
    "results_df_custom_fr = run_optimization_test(\n",
    "    opt_method='FR',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Fletcher-Reeves):\")\n",
    "display(results_df_custom_fr)\n",
    "\n",
    "# ATTENTION! Running _long_: 4:50 for 10000 iters\n",
    "results_df_rosenbrock_pr = run_optimization_test(\n",
    "    opt_method='PR',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "   use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (Polak-Ribiere):\")\n",
    "display(results_df_rosenbrock_pr)\n",
    "\n",
    "# ATTENTION! Running _long_\n",
    "results_df_custom_pr = run_optimization_test(\n",
    "    opt_method='PR',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (Polak-Ribiere):\")\n",
    "display(results_df_custom_pr)\n",
    "\n",
    "# ATTENTION! Running _long_: 4:50 for 10000 iters\n",
    "results_df_rosenbrock_bfgs = run_optimization_test(\n",
    "    opt_method='BFGS',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (BFGS):\")\n",
    "display(results_df_rosenbrock_bfgs)\n",
    "\n",
    "# ATTENTION! Running _long_\n",
    "results_df_custom_bfgs = run_optimization_test(\n",
    "    opt_method='BFGS',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (BFGS):\")\n",
    "display(results_df_custom_bfgs)\n",
    "\n",
    "results_df_rosenbrock_sr1 = run_optimization_test(\n",
    "    opt_method='SR1',\n",
    "    func=safe_rosenbrock,\n",
    "    x0=initial_points_rosenbrock,\n",
    "    known_solution=known_solution_rosenbrock,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Rosenbrock Function (SR1):\")\n",
    "display(results_df_rosenbrock_bfgs)\n",
    "\n",
    "results_df_custom_sr1 = run_optimization_test(\n",
    "    opt_method='SR1',\n",
    "    func=custom_function,\n",
    "    x0=initial_points_custom,\n",
    "    known_solution=known_solution_custom,\n",
    "    use_grad_approx=True,\n",
    "    use_hess_approx=False,\n",
    "    max_iter=10000,\n",
    "    tol=1e-6\n",
    ")\n",
    "display(\"Results for Custom Function (SR1):\")\n",
    "display(results_df_custom_bfgs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c10bcb05d3c1",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "The results from the optimization tests on both the Rosenbrock and Custom functions illustrate key differences in the convergence behavior of the different methods. For example, the Fletcher-Reeves method showed faster convergence on the Rosenbrock function from closer initial points to the global minimum, while Polak-Ribiere was more robust to poor initial conditions on the Custom function.\n",
    "\n",
    "Even more surprising, some methods reached their optimization targets in reasonably few iterations where others required extraordinary amounts of runtime and resources.\n",
    "\n",
    "These findings suggest that the choice of method and starting point can significantly impact the efficiency and success of optimization, especially in complex or ill-conditioned problem spaces. The choice of whether to approximate derivatives seems to have had comparably little impact, though.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bonus Assignment\n",
    "<h3>Goal: Outperform NM with QN</h3>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74597fe39d4fcca0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Function\n",
    "$$\n",
    "f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ba5b508eee2eb22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9e2102cb665fab1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bonus_f(x):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "\n",
    "def bonus_grad_f(x):\n",
    "    df_dx = 2 * (x[0]**2 + x[1] - 11) * 2 * x[0] + 2 * (x[0] + x[1]**2 - 7)\n",
    "    df_dy = 2 * (x[0]**2 + x[1] - 11) * 1 + 2 * (x[0] + x[1]**2 - 7) * 2 * x[1]\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def bonus_hessian_f(x):\n",
    "    d2f_dx2 = 2 * (2 * x[0]) * (2 * x[0]**2 + 2 * x[1] - 11) + 2 * 2 * (x[0] + x[1]**2 - 7)\n",
    "    d2f_dy2 = 2 * (2 * x[0]**2 + 2 * x[1] - 11) * (1) + 2 * 2 * (x[0] + x[1]**2 - 7) * (2 * x[1])\n",
    "    d2f_dxdy = 2 * (2 * x[0]) * (1) + 2 * 2 * (x[0] + x[1]**2 - 7) * (2 * x[1])\n",
    "    return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n",
    "\n",
    "def bonus_backtracking_line_search(f, x, grad, p, alpha=0.3, beta=0.5):\n",
    "    t = 1.0\n",
    "    while f(x + t * p) > f(x) + alpha * t * np.dot(grad, p):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "def bonus_newton_method(f, grad, hessian, x0, max_iter=1000, tol=1e-6):\n",
    "    x = x0.copy()\n",
    "    for i in range(max_iter):\n",
    "        grad_x = grad(x)\n",
    "        if np.linalg.norm(grad_x) < tol:\n",
    "            break\n",
    "        hess_x = hessian(x)\n",
    "        try:\n",
    "            p = np.linalg.solve(hess_x, -grad_x)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break  # In case Hessian is singular, break\n",
    "        t = bonus_backtracking_line_search(f, x, grad_x, p)\n",
    "        x += t * p\n",
    "    return x, np.linalg.norm(grad_x), i\n",
    "\n",
    "def bonus_bfgs_update(H, s, y):\n",
    "    ys = np.dot(y, s)\n",
    "    if ys < 1e-10:  # Prevent division by zero or very small values\n",
    "        return H\n",
    "    rho = 1.0 / ys\n",
    "    I = np.eye(len(H))\n",
    "    V = I - rho * np.outer(s, y)\n",
    "    H = V.T @ H @ V + rho * np.outer(s, s)\n",
    "    return H\n",
    "\n",
    "def bonus_quasi_newton_method(f, grad, x0, max_iter=1000, tol=1e-6):\n",
    "    x = x0.copy()\n",
    "    n = len(x)\n",
    "    H = np.eye(n)\n",
    "    for i in range(max_iter):\n",
    "        grad_x = grad(x)\n",
    "        if np.linalg.norm(grad_x) < tol:\n",
    "            break\n",
    "        p = -np.dot(H, grad_x)\n",
    "        t = bonus_backtracking_line_search(f, x, grad_x, p)\n",
    "        s = t * p\n",
    "        x_next = x + s\n",
    "        y = grad(x_next) - grad_x\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            H = bfgs_update(H, s, y)\n",
    "        x = x_next\n",
    "    return x, np.linalg.norm(grad(x)), i\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b02d56d4f1349e94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e287ad39a99f1c05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x0_list = [(1.0, 1.0), (1.2, 1.2), (-1.2, 1), (0.2, 0.8)]\n",
    "for i, x0 in enumerate(x0_list):\n",
    "    print(f\"Starting point {i+1}: {x0}\")\n",
    "    x, grad_norm, num_iter = bonus_newton_method(f, bonus_grad_f, bonus_hessian_f, np.array(x0))\n",
    "    print(f\"Newton Method: Iterations: {num_iter}, Final iterate: {x}, Gradient norm: {grad_norm}\")\n",
    "\n",
    "    x, grad_norm, num_iter = bonus_quasi_newton_method(f, bonus_grad_f, np.array(x0))\n",
    "    print(f\"Quasi-Newton Method: Iterations: {num_iter}, Final iterate: {x}, Gradient norm: {grad_norm}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8358d7cffcf9f469"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Our Interpretation\n",
    "\n",
    "Based on the optimization results for different starting points, the performance of Newton's method and Quasi-Newton method can be compared.\n",
    "\n",
    "Starting with the initial point (1.0, 1.0), Newton's method required 999 iterations to (still not) converge to a final iterate of (-0.94897959, -2.45918367) with a gradient norm of 44.335083754394404, while the Quasi-Newton method achieved convergence in just 15 iterations, reaching a final iterate of (3.0, 2.00000001) with a much smaller gradient norm of 4.390315402132542e-07.\n",
    "\n",
    "Similarly, at starting point (1.2, 1.2), Newton's method reached a final iterate of converged in 6 iterations to (3.0, 2.0) with a negligible gradient norm of 2.6752441484954133e-11, whereas the Quasi-Newton method required 17 iterations to converge to (3.0, 1.99999999) with a slightly higher gradient norm of 7.085236158428695e-07.\n",
    "\n",
    "For the starting point (-1.2, 1.0), Newton's method again needed 999 iterations to (still not) converge to (-1.2, 1.0) with a gradient norm of 53.11210543746123, while the Quasi-Newton method achieved convergence in just 11 iterations, reaching (-2.80511809, 3.13131251) with a gradient norm of 3.8978054414003885e-07.\n",
    "\n",
    "Finally, at (0.2, 0.8), Newton's method required 999 iterations to (still not converge and) reach (-0.59097274, -1.66551889) with a gradient norm of 20.859414016004283, whereas the Quasi-Newton method converged in 20 iterations to (3.00000001, 1.99999998) with a similar gradient norm of 6.631955937915146e-07.\n",
    "\n",
    "Overall, the Quasi-Newton method dominantly outperformed Newton's method in terms of the number of iterations required for convergence and the achieved gradient norm across different starting points.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1a5a0ea0613ac4a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
