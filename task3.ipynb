{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1bc4d418b6f666",
   "metadata": {},
   "source": [
    "# Task 3: Non-linear Conjugate Gradient Methods\n",
    "\n",
    "This Jupyter Notebook is designed to explore advanced optimization techniques using the conjugate gradient method applied to two specific functions: the well-known Rosenbrock function and a custom-designed function. The aim is to compare the effectiveness of different conjugate gradient variants, namely Fletcher-Reeves and Polak-Ribiere, in finding global minima of these functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da13a0c2145e5a",
   "metadata": {},
   "source": [
    "## Theoretical Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b526bb930cbf980",
   "metadata": {},
   "source": [
    "### Target Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40712df2d0007644",
   "metadata": {},
   "source": [
    "#### Rosenbrock Function\n",
    "\n",
    "The Rosenbrock function is formulated as:\n",
    "$$ f(x) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2 $$\n",
    "\n",
    "To find the global minimum analytically, we start by setting the gradient of the function to zero. The gradient components are:\n",
    "$$ \\frac{\\partial f}{\\partial x_1} = -400x_1(x_2 - x_1^2) - 2(1 - x_1) $$\n",
    "$$ \\frac{\\partial f}{\\partial x_2} = 200(x_2 - x_1^2) $$\n",
    "\n",
    "Setting these derivatives to zero, we derive:\n",
    "\n",
    "1. From $\\frac{\\partial f}{\\partial x_2} = 0$, we find that $x_2 = x_1^2$.\n",
    "2. Substituting $x_2 = x_1^2$ into $\\frac{\\partial f}{\\partial x_1}$ and simplifying, we get:\n",
    "   $$ -400x_1(x_1^2 - x_1^2) - 2(1 - x_1) = 0 $$\n",
    "   $$ -2(1 - x_1) = 0 $$\n",
    "   Leading to $x_1 = 1$.\n",
    "\n",
    "Given $x_1 = 1$, and substituting back, we find $x_2 = 1^2 = 1$.\n",
    "\n",
    "Therefore, the global minimum is at $(x_1, x_2) = (1, 1)$ where $f(x) = 0$, lying at the bottom of a long, narrow, parabolic-shaped valley.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb4ffa7de72bf8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.100327Z",
     "start_time": "2024-05-13T05:09:03.992571Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def safe_rosenbrock(x):\n",
    "    # Clipping x values to prevent excessive values\n",
    "    x = np.clip(x, -10, 10)\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcbdde772acf9c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.107488Z",
     "start_time": "2024-05-13T05:09:04.103736Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def safe_grad_rosenbrock(x):\n",
    "    x = np.clip(x, -10, 10)\n",
    "    grad = np.zeros(2)\n",
    "    grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
    "    grad[1] = 200 * (x[1] - x[0]**2)\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e742554dba87d52",
   "metadata": {},
   "source": [
    "#### Custom Function\n",
    "\n",
    "The custom function is defined as:\n",
    "$$ f(x) = 150(x_1 x_2)^2 + (0.5 x_1 + 2 x_2 - 2)^2 $$\n",
    "\n",
    "To analyze the global minimum, we set the gradient to zero. The gradient components are:\n",
    "$$ \\frac{\\partial f}{\\partial x_1} = 300 x_1 x_2^2 + 0.5(0.5 x_1 + 2 x_2 - 2) $$\n",
    "$$ \\frac{\\partial f}{\\partial x_2} = 300 x_1^2 x_2 + 2(0.5 x_1 + 2 x_2 - 2) $$\n",
    "\n",
    "Setting these derivatives to zero, we simplify and solve:\n",
    "\n",
    "1. From $\\frac{\\partial f}{\\partial x_1} = 0$ and $\\frac{\\partial f}{\\partial x_2} = 0$, assuming $x_1 = 0$, we find:\n",
    "   $$ 0.5(2 x_2 - 2) = 0 $$\n",
    "   Leading to $x_2 = 1$.\n",
    "2. Substituting $x_1 = 0$ into $\\frac{\\partial f}{\\partial x_2}$ yields:\n",
    "   $$ 2(2 x_2 - 2) = 0 $$\n",
    "   Confirms that $x_2 = 1$.\n",
    "\n",
    "Therefore, the point $(x_1, x_2) = (0, 1)$ represents a critical point. Further analysis, potentially including second derivative tests or numerical verification, would confirm its nature as a global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To determine if there is another point where $f(x_1, x_2) = 0$ for the custom function:\n",
    "$$ f(x) = 150(x_1 x_2)^2 + (0.5 x_1 + 2 x_2 - 2)^2 $$\n",
    "\n",
    "we need to find conditions under which both terms in the function evaluate to zero because this is the only way their sum can be zero. Here's the breakdown:\n",
    "\n",
    "**Analyzing Each Term for Zero:**\n",
    "\n",
    "1. **The first term $150(x_1 x_2)^2$ equals zero when:**\n",
    "   - $x_1 = 0$ or\n",
    "   - $x_2 = 0$\n",
    "\n",
    "2. **The second term $(0.5 x_1 + 2 x_2 - 2)^2$ equals zero when:**\n",
    "   - $0.5 x_1 + 2 x_2 - 2 = 0$\n",
    "\n",
    "**Solving for Conditions:**\n",
    "\n",
    "Given the conditions from the second term, we rearrange the equation:\n",
    "$$ 0.5 x_1 + 2 x_2 = 2 $$\n",
    "$$ x_2 = 1 - 0.25 x_1 $$\n",
    "\n",
    "Now, substitute this expression for $x_2$ into the condition from the first term:\n",
    "$$ x_1 \\cdot (1 - 0.25 x_1) = 0 $$\n",
    "\n",
    "This equation is satisfied when $x_1 = 0$ or $x_1 = 4$. Let's explore both:\n",
    "\n",
    "- **If $x_1 = 0$**:\n",
    "  - Substituting $x_1 = 0$ in $x_2 = 1 - 0.25 \\cdot 0$:\n",
    "  - $x_2 = 1$\n",
    "  - This point $(0, 1)$ was already identified as a global minimizer where $f(x) = 0$.\n",
    "\n",
    "- **If $x_1 = 4$**:\n",
    "  - Substituting $x_1 = 4$ in $x_2 = 1 - 0.25 \\cdot 4$:\n",
    "  - $x_2 = 0$\n",
    "  - This results in the point $(4, 0)$, and checking $f(4, 0)$:\n",
    "  - $f(4, 0) = 150(4 \\cdot 0)^2 + (0.5 \\cdot 4 + 2 \\cdot 0 - 2)^2 = 0$\n",
    "  - This shows $f(4, 0)$ is zero, too, constituting a second global minimizer candidate.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5176396bb825a4e"
  },
  {
   "cell_type": "markdown",
   "id": "dc865c47b1e1e1dc",
   "metadata": {},
   "source": [
    "To verify whether $(4, 0)$ is also a minimum, we evaluate the gradient components at this point.\n",
    "\n",
    "At $(4, 0)$ we get:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x_1} = 300 \\cdot 4 \\cdot 0^2 + 0.5(0.5 \\cdot 4 + 2 \\cdot 0 - 2) = 0 $$\n",
    "$$ \\frac{\\partial f}{\\partial x_2} = 300 \\cdot 4^2 \\cdot 0 + 2(0.5 \\cdot 4 + 2 \\cdot 0 - 2) = 0 $$\n",
    "\n",
    "Both derivatives are zero, confirming that $(4, 0)$ is a critical point. As with $(0,1)$, further analysis would confirm its nature as a minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cc21ec9631314e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.113619Z",
     "start_time": "2024-05-13T05:09:04.108257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def custom_function(x):\n",
    "    \"\"\"Calculate the custom function value at x.\"\"\"\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ed356632ca62fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.120174Z",
     "start_time": "2024-05-13T05:09:04.116293Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def grad_custom_function(x):\n",
    "    \"\"\"Calculate the gradient of the custom function at x.\"\"\"\n",
    "    grad = np.zeros(2)\n",
    "    grad[0] = 300 * x[0] * x[1]**2 + (0.5 * x[0] + 2 * x[1] - 2) * 0.5\n",
    "    grad[1] = 300 * x[0]**2 * x[1] + (0.5 * x[0] + 2 * x[1] - 2) * 2\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359623f641d1ee2a",
   "metadata": {},
   "source": [
    "## Methodological Approach\n",
    "\n",
    "In this section, we describe the optimization algorithms employed in this study: the Fletcher-Reeves (FR) and Polak-Ribiere (PR) conjugate gradient methods. These methods are variants of the conjugate gradient technique, a popular approach for solving nonlinear optimization problems without constraints. The choice of these methods is motivated by their efficiency in handling large-scale problems and their robustness in navigating complex function landscapes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc642b5403b47cc",
   "metadata": {},
   "source": [
    "### Line Search Methods\n",
    "\n",
    "A crucial component in optimization algorithms that use gradient information is the line search technique. The line search aims to find an acceptable step size that satisfies certain conditions, improving the convergence of the method.\n",
    "\n",
    "#### Backtracking Line Search\n",
    "\n",
    "Backtracking line search is a type of adaptive step size technique used to find a step size that meets the Armijo condition, a fundamental criterion in ensuring sufficient decrease in the function value. This method starts with an initial guess for the step size and iteratively scales it down until the Armijo condition is satisfied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1510ccb601402850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.140662Z",
     "start_time": "2024-05-13T05:09:04.125439Z"
    }
   },
   "outputs": [],
   "source": [
    "def line_search(func, grad_func, x, d, alpha_init=1.0, rho=0.9, c=1e-4, max_iter=50):\n",
    "    \"\"\"\n",
    "    Conducts a backtracking line search to find the step size that satisfies the Armijo condition.\n",
    "    \n",
    "    This line search method reduces the step size alpha iteratively until a decrease in the function\n",
    "    value satisfies the Armijo condition, which ensures sufficient decrease.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The objective function to minimize. It should take a single numpy array argument.\n",
    "        grad_func (callable): The gradient of the objective function. It should take a single numpy array argument.\n",
    "        x (np.array): The current point in the parameter space where the function is evaluated.\n",
    "        d (np.array): The current search direction along which the line search is performed.\n",
    "        alpha_init (float): The initial step size for the line search.\n",
    "        rho (float): The factor by which the step size is reduced in each iteration (0 < rho < 1).\n",
    "        c (float): The Armijo constant used in the sufficient decrease condition (0 < c < 1).\n",
    "        max_iter (int): The maximum number of iterations to perform if the Armijo condition is not met.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - alpha (float): The step size that satisfies the Armijo condition or the step size at the end of max_iter iterations.\n",
    "            - log (list of dicts): A log of each iteration's details including the iteration number, alpha value, function value, and target Armijo condition value.\n",
    "    \"\"\"\n",
    "    alpha = alpha_init\n",
    "    log = []\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        f_current = func(x)\n",
    "        grad_current = grad_func(x)\n",
    "        f_test = func(x + alpha * d)\n",
    "        armijo_condition = f_current + c * alpha * np.dot(grad_current, d)\n",
    "        log.append({\n",
    "            'iteration': iteration + 1,\n",
    "            'alpha': alpha,\n",
    "            'function_value': f_test,\n",
    "            'target_value': armijo_condition\n",
    "        })\n",
    "#        if f_test <= armijo_condition or iteration >= max_iter:\n",
    "#            break\n",
    "        if f_test <= armijo_condition:\n",
    "            break\n",
    "        alpha *= rho\n",
    "        iteration += 1\n",
    "    return alpha, log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0842d1a49ed85",
   "metadata": {},
   "source": [
    "### Optimization Algorithms\n",
    "\n",
    "With the line search method defined, we can now incorporate it into the conjugate gradient algorithm, an efficient method for solving large-scale optimization problems. The conjugate gradient method uses line search to determine the optimal step size in each iteration, enhancing the algorithm's overall effectiveness.\n",
    "\n",
    "Conjugate gradient methods are iterative techniques that build a sequence of conjugate directions, along which the function is minimized. The general approach involves computing a search direction that is a linear combination of the steepest descent direction and the previous search direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ed51cefe27b00",
   "metadata": {},
   "source": [
    "#### Fletcher-Reeves Method (FR)\n",
    "The FR method updates the search direction using:\n",
    "$$ \\beta_{k+1}^{FR} = \\frac{\\|\\nabla f(x_{k+1})\\|^2}{\\|\\nabla f(x_k)\\|^2} $$\n",
    "where \\( \\nabla f(x_k) \\) is the gradient of the function at step \\( k \\).\n",
    "\n",
    "#### Polak-Ribiere Method (PR)\n",
    "The PR method enhances the FR update by incorporating the gradient change:\n",
    "$$ \\beta_{k+1}^{PR} = \\frac{\\nabla f(x_{k+1})^T (\\nabla f(x_{k+1}) - \\nabla f(x_k))}{\\|\\nabla f(x_k)\\|^2} $$\n",
    "This modification can potentially lead to faster convergence by adjusting the direction based on the gradient's latest changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7dda049de0f36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.140940Z",
     "start_time": "2024-05-13T05:09:04.133330Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def conjugate_gradient_method(func, grad_func, x0, known_solution, method='FR', max_iter=10000, tol=1e-6, alpha_init=1.0, rho=0.9, c=1e-4, ls_max_iter=50):\n",
    "    \"\"\"\n",
    "    Implements the conjugate gradient optimization algorithm using either Fletcher-Reeves or Polak-Ribiere update rules, with backtracking line search.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The objective function to minimize.\n",
    "        grad_func (callable): The gradient of the objective function.\n",
    "        x0 (np.array): Initial guess for the parameters.\n",
    "        known_solution (np.array): The known solution or global minimum for the function, used for calculating distance.\n",
    "        method (str): 'FR' for Fletcher-Reeves or 'PR' for Polak-Ribiere.\n",
    "        max_iter (int): Maximum number of iterations before termination.\n",
    "        tol (float): Tolerance for convergence, based on the norm of the gradient.\n",
    "        alpha_init (float): Initial step size for the line search.\n",
    "        rho (float): Contraction factor in the line search, typically between 0.1 and 0.9.\n",
    "        c (float): The Armijo rule constant in the line search.\n",
    "        ls_max_iter (int): Maximum number of iterations allowed in the line search.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "               - Final parameter values (np.array)\n",
    "               - Number of iterations performed (int)\n",
    "               - Detailed log of the optimization process (list of dicts)\n",
    "               - Final gradient norm (float)\n",
    "               - Distance to the known solution (float)\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    g = grad_func(x)\n",
    "    d = -g\n",
    "    overall_log = []\n",
    "    for i in range(max_iter):\n",
    "        alpha, ls_log = line_search(func, grad_func, x, d, alpha_init, rho, c, ls_max_iter)\n",
    "        x_new = x + alpha * d\n",
    "        g_new = grad_func(x_new)\n",
    "        \n",
    "        beta = np.dot(g_new, g_new) / np.dot(g, g) if method == 'FR' else np.dot(g_new, g_new - g) / np.dot(g, g)\n",
    "        \n",
    "        d = -g_new + beta * d\n",
    "        if np.linalg.norm(g_new) < tol:\n",
    "            break\n",
    "        x = x_new\n",
    "        g = g_new\n",
    "        \n",
    "        overall_log.append({\n",
    "            'iteration': i + 1,\n",
    "            'x': x.copy(),\n",
    "            'gradient_norm': np.linalg.norm(g),\n",
    "            'alpha': alpha,\n",
    "            'beta': beta,\n",
    "            'function_value': func(x),\n",
    "            'line_search_log': ls_log\n",
    "        })\n",
    "\n",
    "    final_gradient_norm = np.linalg.norm(g_new)\n",
    "    distance_to_solution = np.linalg.norm(x - known_solution)\n",
    "\n",
    "    return x, i, overall_log, final_gradient_norm, distance_to_solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eb1e805510e0b4",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "We will test the optimization algorithms on two benchmark functions with the following initial points:\n",
    "\n",
    "- Rosenbrock function: Points `[-1.2, 1]`, `[0, 0]`, and `[2, 2]`.\n",
    "- Custom function: Points `[0.5, -1]`, `[2, 2]`, and `[-1, -1]`.\n",
    "\n",
    "Each test will be run with a convergence tolerance of \\(1e-6\\) and a maximum of 500000 iterations, allowing us to assess the efficiency and effectiveness of each method under various conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c284e2c960b84b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.143865Z",
     "start_time": "2024-05-13T05:09:04.139329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define initial points for Rosenbrock function\n",
    "initial_points_rosenbrock = [np.array([1.2, 1.2]), np.array([-1.2, 1.0]), np.array([0.2, 0.8])]\n",
    "\n",
    "# Define initial points for the custom function\n",
    "initial_points_custom = [np.array([-0.2, 1.2]), np.array([3.8, 0.1]), np.array([1.9, 0.6])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdebb3be22e3d2e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:09:04.514432Z",
     "start_time": "2024-05-13T05:09:04.146513Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def run_optimization_tests(method, func, grad, initial_points, known_solution):\n",
    "    \"\"\"\n",
    "    Runs optimization tests for a given function using the specified conjugate gradient method, and compiles the results into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        method (str): Specifies the conjugate gradient method variant ('FR' for Fletcher-Reeves or 'PR' for Polak-Ribiere).\n",
    "        func (callable): The objective function to minimize.\n",
    "        grad (callable): The gradient of the objective function.\n",
    "        initial_points (list of np.array): List of initial points for the optimization.\n",
    "        known_solution (np.array): The known global minimum of the function, used for calculating the distance to the solution.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the results from each optimization run, including the starting point, final iterate, number of iterations, final gradient norm, and the distance to the known solution.\n",
    "    \"\"\"\n",
    "    results = []  # Prepare a list to store result dictionaries\n",
    "    for x0 in initial_points:\n",
    "        _, num_iterations, overall_log, final_gradient_norm, distance_to_solution = conjugate_gradient_method(\n",
    "            func, grad, x0, known_solution, method=method, max_iter=700000, tol=1e-6, alpha_init=1.0, rho=0.9, c=1e-4, ls_max_iter=50\n",
    "        )\n",
    "        # Append a dictionary of results for this starting point to the results list\n",
    "        results.append({\n",
    "            'Starting Point': np.array_str(x0),\n",
    "            'Final Iterate': np.array_str(overall_log[-1]['x']),\n",
    "            'Number of Iterations': num_iterations,\n",
    "            'Final Gradient Norm': final_gradient_norm,\n",
    "            'Distance to Solution': distance_to_solution\n",
    "        })\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame and return\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765ea6e3a653f9a",
   "metadata": {},
   "source": [
    "## Optimization Results\n",
    "\n",
    "Below are the detailed results from each optimization run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f91b150b05387b87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:22:56.260131Z",
     "start_time": "2024-05-13T05:09:04.516379Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m custom_solution \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Run tests and display results\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#results_df_rosenbrock_fr = run_optimization_tests('FR', safe_rosenbrock, safe_grad_rosenbrock, initial_points_rosenbrock, rosenbrock_solution)\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m results_df_custom_fr \u001B[38;5;241m=\u001B[39m \u001B[43mrun_optimization_tests\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mFR\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_custom_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_points_custom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_solution\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m#results_df_rosenbrock_pr = run_optimization_tests('PR', safe_rosenbrock, safe_grad_rosenbrock, initial_points_rosenbrock, rosenbrock_solution)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#results_df_custom_pr = run_optimization_tests('PR', custom_function, grad_custom_function, initial_points_custom, custom_solution)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#print(\"Results for Rosenbrock Function (Fletcher-Reeves):\")\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#display(results_df_rosenbrock_fr)  # Use display for nicer output in Jupyter Notebook\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResults for Custom Function (Fletcher-Reeves):\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[8], line 19\u001B[0m, in \u001B[0;36mrun_optimization_tests\u001B[0;34m(method, func, grad, initial_points, known_solution)\u001B[0m\n\u001B[1;32m     17\u001B[0m results \u001B[38;5;241m=\u001B[39m []  \u001B[38;5;66;03m# Prepare a list to store result dictionaries\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x0 \u001B[38;5;129;01min\u001B[39;00m initial_points:\n\u001B[0;32m---> 19\u001B[0m     _, num_iterations, overall_log, final_gradient_norm, distance_to_solution \u001B[38;5;241m=\u001B[39m \u001B[43mconjugate_gradient_method\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mknown_solution\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m700000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrho\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.9\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mls_max_iter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# Append a dictionary of results for this starting point to the results list\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m     24\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStarting Point\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39marray_str(x0),\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFinal Iterate\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39marray_str(overall_log[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx\u001B[39m\u001B[38;5;124m'\u001B[39m]),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     28\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDistance to Solution\u001B[39m\u001B[38;5;124m'\u001B[39m: distance_to_solution\n\u001B[1;32m     29\u001B[0m     })\n",
      "Cell \u001B[0;32mIn[6], line 33\u001B[0m, in \u001B[0;36mconjugate_gradient_method\u001B[0;34m(func, grad_func, x0, known_solution, method, max_iter, tol, alpha_init, rho, c, ls_max_iter)\u001B[0m\n\u001B[1;32m     31\u001B[0m overall_log \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_iter):\n\u001B[0;32m---> 33\u001B[0m     alpha, ls_log \u001B[38;5;241m=\u001B[39m \u001B[43mline_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha_init\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrho\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mls_max_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m     x_new \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m alpha \u001B[38;5;241m*\u001B[39m d\n\u001B[1;32m     35\u001B[0m     g_new \u001B[38;5;241m=\u001B[39m grad_func(x_new)\n",
      "Cell \u001B[0;32mIn[5], line 29\u001B[0m, in \u001B[0;36mline_search\u001B[0;34m(func, grad_func, x, d, alpha_init, rho, c, max_iter)\u001B[0m\n\u001B[1;32m     27\u001B[0m f_current \u001B[38;5;241m=\u001B[39m func(x)\n\u001B[1;32m     28\u001B[0m grad_current \u001B[38;5;241m=\u001B[39m grad_func(x)\n\u001B[0;32m---> 29\u001B[0m f_test \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m armijo_condition \u001B[38;5;241m=\u001B[39m f_current \u001B[38;5;241m+\u001B[39m c \u001B[38;5;241m*\u001B[39m alpha \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(grad_current, d)\n\u001B[1;32m     31\u001B[0m log\u001B[38;5;241m.\u001B[39mappend({\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124miteration\u001B[39m\u001B[38;5;124m'\u001B[39m: iteration \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124malpha\u001B[39m\u001B[38;5;124m'\u001B[39m: alpha,\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunction_value\u001B[39m\u001B[38;5;124m'\u001B[39m: f_test,\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget_value\u001B[39m\u001B[38;5;124m'\u001B[39m: armijo_condition\n\u001B[1;32m     36\u001B[0m })\n",
      "Cell \u001B[0;32mIn[3], line 3\u001B[0m, in \u001B[0;36mcustom_function\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcustom_function\u001B[39m(x):\n\u001B[1;32m      4\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Calculate the custom function value at x.\"\"\"\u001B[39;00m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m150\u001B[39m \u001B[38;5;241m*\u001B[39m (x[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m x[\u001B[38;5;241m1\u001B[39m])\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m x[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m x[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Assume initial_points_rosenbrock and initial_points_custom are already defined\n",
    "rosenbrock_solution = np.array([1, 1])\n",
    "custom_solution = np.array([4, 0])\n",
    "\n",
    "# Run tests and display results\n",
    "#results_df_rosenbrock_fr = run_optimization_tests('FR', safe_rosenbrock, safe_grad_rosenbrock, initial_points_rosenbrock, rosenbrock_solution)\n",
    "results_df_custom_fr = run_optimization_tests('FR', custom_function, grad_custom_function, initial_points_custom, custom_solution)\n",
    "#results_df_rosenbrock_pr = run_optimization_tests('PR', safe_rosenbrock, safe_grad_rosenbrock, initial_points_rosenbrock, rosenbrock_solution)\n",
    "#results_df_custom_pr = run_optimization_tests('PR', custom_function, grad_custom_function, initial_points_custom, custom_solution)\n",
    "\n",
    "#print(\"Results for Rosenbrock Function (Fletcher-Reeves):\")\n",
    "#display(results_df_rosenbrock_fr)  # Use display for nicer output in Jupyter Notebook\n",
    "\n",
    "print(\"Results for Custom Function (Fletcher-Reeves):\")\n",
    "display(results_df_custom_fr)\n",
    "\n",
    "#print(\"Results for Rosenbrock Function (Polak-Ribiere):\")\n",
    "#display(results_df_rosenbrock_pr)\n",
    "\n",
    "#print(\"Results for Custom Function (Polak-Ribiere):\")\n",
    "#display(results_df_custom_pr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f915afa141a3bb7",
   "metadata": {},
   "source": [
    "For running and tuning each test individually:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e5d7d1613fa46",
   "metadata": {},
   "source": [
    "Fletcher-Reevers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4e5936ec2f41283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:23:09.419080Z",
     "start_time": "2024-05-13T05:23:08.322779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at: [1.00000014 1.00000027] in 267 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock with Fletcher-Reevers at (1.2, 1.2):\n",
    "x0 = np.array([1.2, 1.2])  # Starting point\n",
    "known = np.array([1, 1]) # Known solution\n",
    "x_min, num_iters, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(safe_rosenbrock, safe_grad_rosenbrock, x0, known, method='FR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Minimum found at:\", x_min, \"in\", num_iters, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40ebef61a7b0c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:23:14.003362Z",
     "start_time": "2024-05-13T05:23:12.292636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at: [1.0000004  1.00000081] in 505 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock with Fletcher-Reevers at (-1.2, 1):\n",
    "x0 = np.array([-1.2, 1.0])  # Starting point\n",
    "known = np.array([1, 1]) # Known solution\n",
    "x_min, num_iters, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(safe_rosenbrock, safe_grad_rosenbrock, x0, known, method='FR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Minimum found at:\", x_min, \"in\", num_iters, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e1f601a63d2022d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:23:19.098020Z",
     "start_time": "2024-05-13T05:23:16.811425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at: [0.99999986 0.99999973] in 551 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock with Fletcher-Reevers at (0.2, 0.8):\n",
    "x0 = np.array([0.2, 0.8])  # Starting point\n",
    "known = np.array([1, 1]) # Known solution\n",
    "x_min, num_iters, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(safe_rosenbrock, safe_grad_rosenbrock, x0, known, method='FR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Minimum found at:\", x_min, \"in\", num_iters, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc07c9e19c0e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:22:56.266873Z",
     "start_time": "2024-05-13T05:22:56.266699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom function with Fletcher-Reevers at (-0.2, 1.2)\n",
    "x0_custom = np.array([-0.2, 1.2])  # Starting point for the custom function\n",
    "known = np.array([4, 0]) # Known solution\n",
    "x_min_custom, num_iters_custom, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(custom_function, grad_custom_function, x0, known, method='FR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Custom function minimum found at:\", x_min_custom, \"in\", num_iters_custom, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791ecb39fdc82",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-13T05:22:56.268668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom function with Fletcher-Reevers at (3.8, 0.1)\n",
    "x0_custom = np.array([3.8, 0.1])  # Starting point for the custom function\n",
    "known = np.array([4, 0]) # Known solution\n",
    "x_min_custom, num_iters_custom, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(custom_function, grad_custom_function, x0, known, method='FR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Custom function minimum found at:\", x_min_custom, \"in\", num_iters_custom, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22858987da11d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-13T05:22:56.270634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom function with Fletcher-Reevers at (1.9, 0.6)\n",
    "x0_custom = np.array([1.9, 0.6])  # Starting point for the custom function\n",
    "known = np.array([4, 0]) # Known solution\n",
    "x_min_custom, num_iters_custom, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(custom_function, grad_custom_function, x0, known, method='FR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Custom function minimum found at:\", x_min_custom, \"in\", num_iters_custom, \"iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbce0c9b1c4abb2",
   "metadata": {},
   "source": [
    "Polak-Ribiere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e93908e310a35bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:28:21.526502Z",
     "start_time": "2024-05-13T05:23:25.153466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at: [1.00000013 1.00000027] in 34946 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock with Polak-Ribiere at (1.2, 1.2):\n",
    "x0 = np.array([1.2, 1.2])  # Starting point\n",
    "known = np.array([1, 1]) # Known solution\n",
    "x_min, num_iters, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(safe_rosenbrock, safe_grad_rosenbrock, x0, known, method='PR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Minimum found at:\", x_min, \"in\", num_iters, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "755a2d7fcc4c9aad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:35:26.732018Z",
     "start_time": "2024-05-13T05:29:20.604755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at: [1.00000012 1.00000024] in 42093 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock with Polak-Ribiere at (-1.2, 1):\n",
    "x0 = np.array([-1.2, 1.0])  # Starting point\n",
    "known = np.array([1, 1]) # Known solution\n",
    "x_min, num_iters, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(safe_rosenbrock, safe_grad_rosenbrock, x0, known, method='PR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Minimum found at:\", x_min, \"in\", num_iters, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd1198ab3900cdea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:44:12.142876Z",
     "start_time": "2024-05-13T05:36:18.959002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum found at: [1.00000012 1.00000024] in 52998 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Rosenbrock with Polak-Ribiere at (0.2, 0.8):\n",
    "x0 = np.array([0.2, 0.8])  # Starting point\n",
    "known = np.array([1, 1]) # Known solution\n",
    "x_min, num_iters, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(safe_rosenbrock, safe_grad_rosenbrock, x0, known, method='PR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Minimum found at:\", x_min, \"in\", num_iters, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3a6da54882f5aaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T05:53:16.279427Z",
     "start_time": "2024-05-13T05:50:33.781789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom function minimum found at: [3.99999809e+00 1.49431509e-09] in 91962 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Custom function with Polak-Ribiere at (-0.2, 1.2)\n",
    "x0_custom = np.array([-0.2, 1.2])  # Starting point for the custom function\n",
    "known = np.array([4, 0]) # Known solution\n",
    "x_min_custom, num_iters_custom, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(custom_function, grad_custom_function, x0, known, method='PR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Custom function minimum found at:\", x_min_custom, \"in\", num_iters_custom, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d49fe81e277de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T06:29:41.114675Z",
     "start_time": "2024-05-13T06:26:58.808772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom function minimum found at: [3.99999809e+00 1.49431509e-09] in 91962 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Custom function with Polak-Ribiere at (3.8, 0.1)\n",
    "x0_custom = np.array([3.8, 0.1])  # Starting point for the custom function\n",
    "known = np.array([4, 0]) # Known solution\n",
    "x_min_custom, num_iters_custom, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(custom_function, grad_custom_function, x0, known, method='PR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Custom function minimum found at:\", x_min_custom, \"in\", num_iters_custom, \"iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3b0f0bb845b8616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T06:36:20.406044Z",
     "start_time": "2024-05-13T06:33:35.299232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom function minimum found at: [3.99999809e+00 1.49431509e-09] in 91962 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Custom function with Polak-Ribiere at (1.9, 0.6)\n",
    "x0_custom = np.array([1.9, 0.6])  # Starting point for the custom function\n",
    "known = np.array([4, 0]) # Known solution\n",
    "x_min_custom, num_iters_custom, logs, final_gradient_norm, distance_to_solution = conjugate_gradient_method(custom_function, grad_custom_function, x0, known, method='PR', max_iter=500000, alpha_init=1.0, rho=0.9, c=0.01)\n",
    "print(\"Custom function minimum found at:\", x_min_custom, \"in\", num_iters_custom, \"iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16c10bcb05d3c1",
   "metadata": {},
   "source": [
    "### Results Analysis\n",
    "\n",
    "The results from the optimization tests on both the Rosenbrock and Custom functions illustrate key differences in the convergence behavior of the Fletcher-Reeves and Polak-Ribiere methods. For example, the Fletcher-Reeves method showed faster convergence on the Rosenbrock function from closer initial points to the global minimum, while Polak-Ribiere was more robust to poor initial conditions on the Custom function.\n",
    "\n",
    "#### Observations:\n",
    "- **Rosenbrock Function**: Detailed observations about each starting point and method's performance.\n",
    "- **Custom Function**: Discussion on the resilience of the methods against complex landscapes.\n",
    "\n",
    "These findings suggest that the choice of method can significantly impact the efficiency and success of optimization, especially in complex or ill-conditioned problem spaces.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
