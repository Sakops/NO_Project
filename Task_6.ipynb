{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fa0779-2587-4d25-8734-bd6cc7334376",
   "metadata": {},
   "source": [
    "# Task 6: Outperforming Newton method with Quasi-Newton methods.\n",
    "\n",
    "In this report, we present the implementation and application of two optimization methods: the Newton method and the BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno) quasi-Newton method. These methods are applied to two test functions: the Rosenbrock function and a custom second function. We will also include detailed logging to trace the execution and compare the performance of these methods in terms of iterations and computation time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a3e5b-1264-4b95-baa9-e7434bffa1f8",
   "metadata": {},
   "source": [
    "## Optimization Methods\n",
    "### Newton Method\n",
    "The Newton method is a second-order optimization technique that uses both the gradient and the Hessian of the function to find the local minima or maxima. It assumes that the function can be locally approximated by a quadratic function, leading to the following iterative update rule:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{k+1} = \\mathbf{x}_k - (\\nabla^2 f(\\mathbf{x}_k))^{-1} \\nabla f(\\mathbf{x}_k)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_k$ is the current iterate, $\\nabla f(\\mathbf{x}_k)$ is the gradient, and $\\nabla^2 f(\\mathbf{x}_k)$ is the Hessian matrix. The method typically converges very quickly if the initial guess is close to the true solution and the function is well-behaved (e.g., strongly convex).\n",
    "\n",
    "However, the Newton method has some limitations:\n",
    "1. **Computation of Hessian**: Calculating the Hessian matrix and its inverse can be computationally expensive, especially for high-dimensional problems.\n",
    "2. **Requirement for a good initial guess**: The method may fail or converge to a non-optimal point if the initial guess is far from the solution or if the function is not well-behaved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6383102f-cb6c-4203-a871-70b4dd26e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def detailed_logging(x, grad, hess_inv, iteration, debug=False):\n",
    "    if debug:\n",
    "        print(f\"Iteration {iteration}: x = {x}, grad = {grad}, hess_inv = {hess_inv}\")\n",
    "\n",
    "def newton_method(f, grad_f, hess_f, x0, tol=1e-8, max_iter=1000, debug=False):\n",
    "    x = x0\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        hess = hess_f(x)\n",
    "        # Solve the linear system Hx = g instead of computing the inverse\n",
    "        try:\n",
    "            step = np.linalg.solve(hess, -grad)\n",
    "        except np.linalg.LinAlgError:\n",
    "            if debug:\n",
    "                print(\"Hessian is singular or nearly singular at iteration\", i)\n",
    "            break  # Exit if the Hessian is singular or nearly singular\n",
    "\n",
    "        x = x + step\n",
    "        detailed_logging(x, grad, np.linalg.inv(hess), i, debug)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            if debug:\n",
    "                print(f\"Solution found at iteration {i+1}\")\n",
    "            return x, i+1\n",
    "    return x, max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c0910-b661-44b1-bb02-d2f1894f9483",
   "metadata": {},
   "source": [
    "### BFGS Method\n",
    "The BFGS method is a quasi-Newton method that builds up an approximation to the inverse Hessian matrix using gradient information only. This method is widely used because it does not require the computation of the Hessian matrix, making it more suitable for large-scale optimization problems. The update rule for the inverse Hessian approximation $\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{B}_{k+1} = \\mathbf{B}_k + \\frac{\\mathbf{y}_k \\mathbf{y}_k^T}{\\mathbf{y}_k^T \\mathbf{s}_k} - \\frac{\\mathbf{B}_k \\mathbf{s}_k \\mathbf{s}_k^T \\mathbf{B}_k}{\\mathbf{s}_k^T \\mathbf{B}_k \\mathbf{s}_k}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ and $\\mathbf{y}_k = \\nabla f(\\mathbf{x}_{k+1}) - \\nabla f(\\mathbf{x}_k)$. The BFGS method iteratively improves the approximation of the inverse Hessian, leading to superlinear convergence.\n",
    "\n",
    "Key advantages of the BFGS method include:\n",
    "1. **No need for the Hessian**: It avoids the explicit calculation of the Hessian matrix, making it more computationally efficient for large problems.\n",
    "2. **Global convergence properties**: It is more robust and often converges more reliably from poor initial guesses compared to the Newton method.\n",
    "\n",
    "However, the BFGS method also has some limitations:\n",
    "1. **Storage requirements**: Storing the approximate Hessian matrix can be memory-intensive for very large problems.\n",
    "2. **Line search dependency**: The efficiency and convergence of the method depend on the effectiveness of the line search algorithm used to determine the step size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd398622-a828-49ad-b5b5-255747a38766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(f, x, p, grad_f, alpha=1.0, beta=0.5, c=1e-4):\n",
    "    while f(x + alpha * p) > f(x) + c * alpha * np.dot(grad_f(x), p):\n",
    "        alpha *= beta  #Reduce step size\n",
    "    return alpha\n",
    "\n",
    "def bfgs_method(f, grad_f, x0, tol=1e-6, max_iter=1000, debug=False):\n",
    "    x = x0\n",
    "    n = len(x)\n",
    "    B = np.eye(n)  #initialize B as the identity matrix of dimension n\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            if debug:\n",
    "                print(f\"Solution found at iteration {i+1}\")\n",
    "            return x, i+1\n",
    "\n",
    "        p = -np.linalg.solve(B, grad)  # Direction to move in\n",
    "        # Apply line search to find the optimal step size\n",
    "        alpha = line_search(f, x, p, grad_f)\n",
    "\n",
    "        x_new = x + alpha * p\n",
    "        grad_new = grad_f(x_new)\n",
    "        \n",
    "        # Update s and y\n",
    "        s = x_new - x\n",
    "        y = grad_new - grad\n",
    "        \n",
    "        # Update the approximation to the Hessian matrix, B\n",
    "        if np.dot(y, s) > 0:  # Only update if curvature condition is satisfied\n",
    "            Bs = B @ s\n",
    "            B += np.outer(y, y) / np.dot(y, s) - np.outer(Bs, Bs) / np.dot(s, Bs)\n",
    "        \n",
    "        x = x_new\n",
    "        \n",
    "        if debug:\n",
    "            detailed_logging(x, grad, np.linalg.inv(B), i)\n",
    "\n",
    "    return x, max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821b092-2d95-427b-9a0b-deab2f83d240",
   "metadata": {},
   "source": [
    "## Test Functions and Starting Points\n",
    "### Rosenbrock Function\n",
    "The Rosenbrock function, also known as the Rosenbrock's valley or Rosenbrock's banana function, is a common test problem for optimization algorithms. It is defined as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = 100 (x_2 - x_1^2)^2 + (1 - x_1)^2\n",
    "$$\n",
    "\n",
    "This function has a narrow, curved valley containing the global minimum. The gradient and Hessian of the Rosenbrock function are given by:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "-400 x_1 (x_2 - x_1^2) - 2 (1 - x_1) \\\\\n",
    "200 (x_2 - x_1^2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "-400 (x_2 - x_1^2) + 800 x_1^2 + 2 & -400 x_1 \\\\\n",
    "-400 x_1 & 200\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Due to its narrow and curved valley, the Rosenbrock function poses a significant challenge for optimization algorithms, making it an excellent candidate for testing the performance and robustness of different methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36ced2d0-0806-4c61-b5ae-5dda9f97e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    return np.array([\n",
    "        -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0]),\n",
    "        200 * (x[1] - x[0]**2)\n",
    "    ])\n",
    "\n",
    "def hess_rosenbrock(x):\n",
    "    return np.array([\n",
    "        [-400 * (x[1] - x[0]**2) + 800 * x[0]**2 + 2, -400 * x[0]],\n",
    "        [-400 * x[0], 200]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52145b11-b132-4c1d-b64c-c9ce4047bce2",
   "metadata": {},
   "source": [
    "### Second Function\n",
    "The second function used for testing is defined as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = 150 (x_1 x_2)^2 + (0.5 x_1 + 2 x_2 - 2)^2\n",
    "$$\n",
    "\n",
    "This function is designed to have multiple minima and different characteristics compared to the Rosenbrock function. The gradient and Hessian of the second function are:\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "300 x_1 x_2^2 + 0.5 (0.5 x_1 + 2 x_2 - 2) \\\\\n",
    "300 x_1^2 x_2 + 2 (0.5 x_1 + 2 x_2 - 2)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x}) = \\begin{bmatrix}\n",
    "300 x_2^2 + 0.25 & 600 x_1 x_2 + 0.5 \\\\\n",
    "600 x_1 x_2 + 0.5 & 300 x_1^2 + 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This function provides a complementary challenge to the Rosenbrock function, allowing us to evaluate the performance of the optimization methods on a function with different properties and a different landscape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38de9c3e-3338-464d-a158-6b140b5590ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_function(x):\n",
    "    return 150 * (x[0] * x[1])**2 + (0.5 * x[0] + 2 * x[1] - 2)**2\n",
    "\n",
    "def grad_second_function(x):\n",
    "    return np.array([\n",
    "        300 * x[0] * x[1]**2 + 0.5 * (0.5 * x[0] + 2 * x[1] - 2),\n",
    "        300 * x[0]**2 * x[1] + 2 * (0.5 * x[0] + 2 * x[1] - 2)\n",
    "    ])\n",
    "\n",
    "def hess_second_function(x):\n",
    "    return np.array([\n",
    "        [300 * x[1]**2 + 0.25, 600 * x[0] * x[1] + 0.5],\n",
    "        [600 * x[0] * x[1] + 0.5, 300 * x[0]**2 + 4]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a7f08-54e0-4090-b7ad-22c3803f4d1d",
   "metadata": {},
   "source": [
    "### Starting Points\n",
    "\n",
    "We will test the optimization methods with the following starting points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb5154c-2b9b-4f45-8e72-794442021c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_points_rosenbrock = [\n",
    "    np.array([1.2, 1.2]), np.array([-1.2, 1]), np.array([0.2, 0.8])\n",
    "]\n",
    "\n",
    "start_points_second_function = [\n",
    "    np.array([-0.2, 1.2]), np.array([3.8, 0.1]), np.array([1.9, 0.6])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99764f-8947-4eb6-833d-c3348730326c",
   "metadata": {},
   "source": [
    "### Running and Comparing Methods\n",
    "The following code runs the Newton and BFGS methods on the defined functions with the specified starting points. It logs the results and compares the performance in terms of iterations and time taken.  \n",
    "The results of the optimization methods are summarized below. Each entry shows the function, method, starting point, found solution, number of iterations, and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02371097-1d6b-497d-a4f0-188df442d6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Function</th>\n",
       "      <th>Method</th>\n",
       "      <th>Starting Point</th>\n",
       "      <th>Solution</th>\n",
       "      <th>Iterations</th>\n",
       "      <th>Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Run 1</th>\n",
       "      <td>Rosenbrock</td>\n",
       "      <td>Newton</td>\n",
       "      <td>[1.2, 1.2]</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.001997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 2</th>\n",
       "      <td>Rosenbrock</td>\n",
       "      <td>BFGS</td>\n",
       "      <td>[1.2, 1.2]</td>\n",
       "      <td>[1.0000000040561998, 1.0000000091122738]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 3</th>\n",
       "      <td>Rosenbrock</td>\n",
       "      <td>Newton</td>\n",
       "      <td>[-1.2, 1.0]</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 4</th>\n",
       "      <td>Rosenbrock</td>\n",
       "      <td>BFGS</td>\n",
       "      <td>[-1.2, 1.0]</td>\n",
       "      <td>[0.9999999952278598, 0.9999999902393124]</td>\n",
       "      <td>35</td>\n",
       "      <td>0.003001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 5</th>\n",
       "      <td>Rosenbrock</td>\n",
       "      <td>Newton</td>\n",
       "      <td>[0.2, 0.8]</td>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 6</th>\n",
       "      <td>Rosenbrock</td>\n",
       "      <td>BFGS</td>\n",
       "      <td>[0.2, 0.8]</td>\n",
       "      <td>[0.9999999994088168, 0.9999999983055982]</td>\n",
       "      <td>20</td>\n",
       "      <td>0.002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 7</th>\n",
       "      <td>Second Function</td>\n",
       "      <td>Newton</td>\n",
       "      <td>[-0.2, 1.2]</td>\n",
       "      <td>[0.35283268987680116, 0.0882081724756321]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.002002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 8</th>\n",
       "      <td>Second Function</td>\n",
       "      <td>BFGS</td>\n",
       "      <td>[-0.2, 1.2]</td>\n",
       "      <td>[1.9510723273165978e-09, 1.000000014983516]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 9</th>\n",
       "      <td>Second Function</td>\n",
       "      <td>Newton</td>\n",
       "      <td>[3.8, 0.1]</td>\n",
       "      <td>[0.35283268990802114, 0.08820817246782824]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.001367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 10</th>\n",
       "      <td>Second Function</td>\n",
       "      <td>BFGS</td>\n",
       "      <td>[3.8, 0.1]</td>\n",
       "      <td>[3.9999999904953767, 3.004652295621832e-11]</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 11</th>\n",
       "      <td>Second Function</td>\n",
       "      <td>Newton</td>\n",
       "      <td>[1.9, 0.6]</td>\n",
       "      <td>[0.35283268985998145, 0.0882081724798473]</td>\n",
       "      <td>12</td>\n",
       "      <td>0.002179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Run 12</th>\n",
       "      <td>Second Function</td>\n",
       "      <td>BFGS</td>\n",
       "      <td>[1.9, 0.6]</td>\n",
       "      <td>[-3.082461616985395e-09, 1.000000131403454]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Function  Method Starting Point  \\\n",
       "Run 1        Rosenbrock  Newton     [1.2, 1.2]   \n",
       "Run 2        Rosenbrock    BFGS     [1.2, 1.2]   \n",
       "Run 3        Rosenbrock  Newton    [-1.2, 1.0]   \n",
       "Run 4        Rosenbrock    BFGS    [-1.2, 1.0]   \n",
       "Run 5        Rosenbrock  Newton     [0.2, 0.8]   \n",
       "Run 6        Rosenbrock    BFGS     [0.2, 0.8]   \n",
       "Run 7   Second Function  Newton    [-0.2, 1.2]   \n",
       "Run 8   Second Function    BFGS    [-0.2, 1.2]   \n",
       "Run 9   Second Function  Newton     [3.8, 0.1]   \n",
       "Run 10  Second Function    BFGS     [3.8, 0.1]   \n",
       "Run 11  Second Function  Newton     [1.9, 0.6]   \n",
       "Run 12  Second Function    BFGS     [1.9, 0.6]   \n",
       "\n",
       "                                           Solution  Iterations  Time (s)  \n",
       "Run 1                                    [1.0, 1.0]           6  0.001997  \n",
       "Run 2      [1.0000000040561998, 1.0000000091122738]          13  0.001002  \n",
       "Run 3                                    [1.0, 1.0]           7  0.000996  \n",
       "Run 4      [0.9999999952278598, 0.9999999902393124]          35  0.003001  \n",
       "Run 5                                    [1.0, 1.0]           6  0.000995  \n",
       "Run 6      [0.9999999994088168, 0.9999999983055982]          20  0.002001  \n",
       "Run 7     [0.35283268987680116, 0.0882081724756321]          19  0.002002  \n",
       "Run 8   [1.9510723273165978e-09, 1.000000014983516]          17  0.001999  \n",
       "Run 9    [0.35283268990802114, 0.08820817246782824]          12  0.001367  \n",
       "Run 10  [3.9999999904953767, 3.004652295621832e-11]           8  0.000000  \n",
       "Run 11    [0.35283268985998145, 0.0882081724798473]          12  0.002179  \n",
       "Run 12  [-3.082461616985395e-09, 1.000000131403454]          17  0.001002  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "#collect results for Rosenbrock function\n",
    "for x0 in start_points_rosenbrock:\n",
    "    start_time = time.time()\n",
    "    solution, iterations = newton_method(rosenbrock, grad_rosenbrock, hess_rosenbrock, x0)\n",
    "    duration = time.time() - start_time\n",
    "    results.append(('Rosenbrock', 'Newton', x0.tolist(), solution.tolist(), iterations, duration))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    solution, iterations = bfgs_method(rosenbrock, grad_rosenbrock, x0)\n",
    "    duration = time.time() - start_time\n",
    "    results.append(('Rosenbrock', 'BFGS', x0.tolist(), solution.tolist(), iterations, duration))\n",
    "\n",
    "#collect results for the second custom function\n",
    "for x0 in start_points_second_function:\n",
    "    start_time = time.time()\n",
    "    solution, iterations = newton_method(second_function, grad_second_function, hess_second_function, x0)\n",
    "    duration = time.time() - start_time\n",
    "    results.append(('Second Function', 'Newton', x0.tolist(), solution.tolist(), iterations, duration))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    solution, iterations = bfgs_method(second_function, grad_second_function, x0)\n",
    "    duration = time.time() - start_time\n",
    "    results.append(('Second Function', 'BFGS', x0.tolist(), solution.tolist(), iterations, duration))\n",
    "\n",
    "#Display results in a Pandas DataFrame\n",
    "index_labels = [f\"Run {i + 1}\" for i in range(len(results))]\n",
    "df_results = pd.DataFrame(results, columns=['Function', 'Method', 'Starting Point', 'Solution', 'Iterations', 'Time (s)'], index=index_labels)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f874ef6-c990-43f7-a12d-6f133dac0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton is slower than Quasi-Newton for the function Rosenbrock and the starting point [1.2, 1.2]\n",
      "Quasi-Newton is slower than Newton for the function Rosenbrock and the starting point [-1.2, 1.0]\n",
      "Quasi-Newton is slower than Newton for the function Rosenbrock and the starting point [0.2, 0.8]\n",
      "Newton is slower than Quasi-Newton for the function Second Function and the starting point [-0.2, 1.2]\n",
      "Newton is slower than Quasi-Newton for the function Second Function and the starting point [3.8, 0.1]\n",
      "Newton is slower than Quasi-Newton for the function Second Function and the starting point [1.9, 0.6]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(df_results), 2):\n",
    "    function_newton = df_results.iloc[i]['Function']\n",
    "    starting_point_newton = df_results.iloc[i]['Starting Point']\n",
    "    time_newton = df_results.iloc[i]['Time (s)']\n",
    "    \n",
    "    function_bfgs = df_results.iloc[i+1]['Function']\n",
    "    starting_point_bfgs = df_results.iloc[i+1]['Starting Point']\n",
    "    time_bfgs = df_results.iloc[i+1]['Time (s)']\n",
    "    \n",
    "    if time_newton > time_bfgs:\n",
    "        print(f\"Newton is slower than Quasi-Newton for the function {function_newton} and the starting point {starting_point_newton}\")\n",
    "    else:\n",
    "        print(f\"Quasi-Newton is slower than Newton for the function {function_bfgs} and the starting point {starting_point_bfgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cb3d71-34d0-4e0b-b6e8-d360915270a5",
   "metadata": {},
   "source": [
    "## Detailed Analysis of Newton vs. BFGS Methods\n",
    "Our Objective was to identify problems to which the solution of the Newton method exhibits slower runtime performance compared to the BFGS quasi-Newton method when applied to two functions, the Rosenbrock function and a second custom function. The analysis of runtime is critical as it reflects the efficiency of the algorithm in practical scenarios, especially in large-scale applications where runtime can significantly impact overall performance.\n",
    "\n",
    "### Experiment Setup\n",
    "- **Functions**: This method leverages the full Hessian matrix to compute updates, offering precise convergence in ideal scenarios but at the cost of increased computational overhead.\n",
    "- **Methods**: As a quasi-Newton method, BFGS approximates the Hessian matrix updates, which can significantly reduce computational demands, particularly beneficial in large-dimensional problems or under conditions where the Hessian is difficult to compute.\n",
    "- **Starting Points**: Multiple starting points were chosen to assess the robustness of the algorithms under varying initial conditions.\n",
    "\n",
    "We added a special technique called a \"line search algorithm\" to the BFGS optimization method. This technique adjusts the step size ùõº, each time the method tries to find the minimum of a function. The step size is how far the method moves along in its search for the lowest point.  \n",
    "The purpose of this line search is to find the most effective step size for each move. This ensures that each step improves the position towards finding the function's minimum value without going too far or not far enough. It's like fine-tuning each step to make sure it's just right.  \n",
    "We used a straightforward approach known as the \"backtracking line search,\" which is based on a simple optimization rule called the Armijo condition. This starts with a guess for the step size and keeps reducing it gradually until the improvement in the function's value is good enough to meet a specific requirement. This requirement involves checking the slope (directional derivative) at the current point and adjusting the step size accordingly.  \n",
    "By carefully adjusting the step size on each iteration, this line search helps the BFGS method to avoid two main problems: overshooting (going too far and missing the minimum) and taking steps that are too small, which can slow down the process. As a result, this adjustment helps the method find the minimum point of the function more quickly and reliably.  \n",
    "\n",
    "This enhancement makes the BFGS method more efficient, especially in tricky optimization problems where finding the right step size isn't straightforward. It's like having a smart assistant that helps the method decide how big each step should be to efficiently reach the goal.  \n",
    "\n",
    "### Results\n",
    "- **Rosenbrock Function**: For the first starting point ([1.2, 1.2]), the Newton's method is performing slower than Quasi-Newton's method. For all the other starting points ([-1.2, 1.0]), and ([0.2, 0.8]), the Newton method was consistently faster in terms of runtime compared to BFGS. This might be attributed to the second-order nature of the Newton method, which, despite the overhead of computing the Hessian, converges in fewer iterations when close to the solution within a well-structured problem like the Rosenbrock function.\n",
    "\n",
    "- **Second Function**: For starting points ([-0.2, 1.2]), ([3.8, 0.1]), and ([1.9, 0.6]), the Newton method is relatively slower than Quas-Newton method. This suggests that in scenarios where the function landscape is complex or ill-conditioned with respect to the initial point, the BFGS method's less computationally intensive update rule can achieve faster convergence in runtime.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Efficiency**: The results clearly indicate that while the Newton method can be highly efficient in terms of iterations due to its use of exact second-order information, its runtime efficiency is highly dependent on the nature of the function and the proximity of the initial point to the solution. The BFGS method, with its approximative approach, offers a significant advantage in scenarios where the Hessian computation is expensive or the function is poorly conditioned.\n",
    "\n",
    "- **Application**: These findings are crucial for applications where runtime is a critical factor. For functions similar to our second test function or in high-dimensional optimization problems, BFGS may be preferred over Newton.\n",
    "\n",
    "### Conclusion\n",
    "The experiment successfully identified specific cases where the BFGS quasi-Newton method outperforms the Newton method in terms of runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ef767-b170-442f-96db-67c4acf37f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
