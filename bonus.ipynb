{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Bonus Assignment</h1>\n",
    "<h3>Goal: Outperform NM with QN</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Our Function:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting point 1: (1.0, 1.0)\n",
      "Newton Method: Iterations: 999, Final iterate: [-0.94897959 -2.45918367], Gradient norm: 44.335083754394404\n",
      "Quasi-Newton Method: Iterations: 15, Final iterate: [3.         2.00000001], Gradient norm: 4.390315402132542e-07\n",
      "\n",
      "Starting point 2: (1.2, 1.2)\n",
      "Newton Method: Iterations: 6, Final iterate: [3. 2.], Gradient norm: 2.6752441484954133e-11\n",
      "Quasi-Newton Method: Iterations: 17, Final iterate: [3.         1.99999999], Gradient norm: 7.085236158428695e-07\n",
      "\n",
      "Starting point 3: (-1.2, 1)\n",
      "Newton Method: Iterations: 999, Final iterate: [-1.2  1. ], Gradient norm: 53.11210543746123\n",
      "Quasi-Newton Method: Iterations: 11, Final iterate: [-2.80511809  3.13131251], Gradient norm: 3.8978054414003885e-07\n",
      "\n",
      "Starting point 4: (0.2, 0.8)\n",
      "Newton Method: Iterations: 999, Final iterate: [-0.59097274 -1.66551889], Gradient norm: 20.859414016004283\n",
      "Quasi-Newton Method: Iterations: 20, Final iterate: [3.00000001 1.99999998], Gradient norm: 6.631955937915146e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "\n",
    "def grad_f(x):\n",
    "    df_dx = 2 * (2 * x[0] * (x[0]**2 + x[1] - 11) + x[0] + x[1]**2 - 7)\n",
    "    df_dy = 2 * (x[0]**2 + 2 * x[1] * (x[0] + x[1]**2 - 7) + x[1] - 11)\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def hessian_f(x):\n",
    "    d2f_dx2 = 4 * (x[0]**2 + x[1] - 11) + 8 * x[0]**2 + 2\n",
    "    d2f_dy2 = 4 * (x[0] + x[1]**2 - 7) + 8 * x[1]**2 + 2\n",
    "    d2f_dxdy = 4 * x[0] + 4 * x[1]\n",
    "    return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n",
    "\n",
    "def backtracking_line_search(f, x, grad, p, alpha=0.3, beta=0.5):\n",
    "    t = 1.0\n",
    "    while f(x + t * p) > f(x) + alpha * t * np.dot(grad, p):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "def newton_method(f, grad, hessian, x0, max_iter=1000, tol=1e-6):\n",
    "    x = x0.copy()\n",
    "    for i in range(max_iter):\n",
    "        grad_x = grad(x)\n",
    "        if np.linalg.norm(grad_x) < tol:\n",
    "            break\n",
    "        hess_x = hessian(x)\n",
    "        try:\n",
    "            p = np.linalg.solve(hess_x, -grad_x)\n",
    "        except np.linalg.LinAlgError:\n",
    "            break  # In case Hessian is singular, break\n",
    "        t = backtracking_line_search(f, x, grad_x, p)\n",
    "        x += t * p\n",
    "    return x, np.linalg.norm(grad_x), i\n",
    "\n",
    "def bfgs_update(H, s, y):\n",
    "    ys = np.dot(y, s)\n",
    "    if ys < 1e-10:  # Prevent division by zero or very small values\n",
    "        return H\n",
    "    rho = 1.0 / ys\n",
    "    I = np.eye(len(H))\n",
    "    V = I - rho * np.outer(s, y)\n",
    "    H = V.T @ H @ V + rho * np.outer(s, s)\n",
    "    return H\n",
    "\n",
    "def quasi_newton_method(f, grad, x0, max_iter=1000, tol=1e-6):\n",
    "    x = x0.copy()\n",
    "    n = len(x)\n",
    "    H = np.eye(n)\n",
    "    for i in range(max_iter):\n",
    "        grad_x = grad(x)\n",
    "        if np.linalg.norm(grad_x) < tol:\n",
    "            break\n",
    "        p = -np.dot(H, grad_x)\n",
    "        t = backtracking_line_search(f, x, grad_x, p)\n",
    "        s = t * p\n",
    "        x_next = x + s\n",
    "        y = grad(x_next) - grad_x\n",
    "        if np.dot(y, s) > 1e-10:\n",
    "            H = bfgs_update(H, s, y)\n",
    "        x = x_next\n",
    "    return x, np.linalg.norm(grad(x)), i\n",
    "\n",
    "x0_list = [(1.0, 1.0), (1.2, 1.2), (-1.2, 1), (0.2, 0.8)]\n",
    "for i, x0 in enumerate(x0_list):\n",
    "    print(f\"Starting point {i+1}: {x0}\")\n",
    "    x, grad_norm, num_iter = newton_method(f, grad_f, hessian_f, np.array(x0))\n",
    "    print(f\"Newton Method: Iterations: {num_iter}, Final iterate: {x}, Gradient norm: {grad_norm}\")\n",
    "\n",
    "    x, grad_norm, num_iter = quasi_newton_method(f, grad_f, np.array(x0))\n",
    "    print(f\"Quasi-Newton Method: Iterations: {num_iter}, Final iterate: {x}, Gradient norm: {grad_norm}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Based on the optimization results for different starting points, the performance of Newton's method and Quasi-Newton method can be compared. Starting with the initial point (1.0, 1.0), Newton's method required 999 iterations to converge to a final iterate of (-0.94897959, -2.45918367) with a gradient norm of 44.335083754394404, while the Quasi-Newton method achieved convergence in just 15 iterations, reaching a final iterate of (3.0, 2.00000001) with a much smaller gradient norm of 4.390315402132542e-07. Similarly, at starting point (1.2, 1.2), Newton's method converged in 6 iterations to (3.0, 2.0) with a negligible gradient norm of 2.6752441484954133e-11, whereas the Quasi-Newton method required 17 iterations to converge to (3.0, 1.99999999) with a slightly higher gradient norm of 7.085236158428695e-07. For the starting point (-1.2, 1.0), Newton's method again needed 999 iterations to converge to (-1.2, 1.0) with a gradient norm of 53.11210543746123, while the Quasi-Newton method achieved convergence in just 11 iterations, reaching (-2.80511809, 3.13131251) with a gradient norm of 3.8978054414003885e-07. Finally, at (0.2, 0.8), Newton's method required 999 iterations to reach (-0.59097274, -1.66551889) with a gradient norm of 20.859414016004283, whereas the Quasi-Newton method converged in 20 iterations to (3.00000001, 1.99999998) with a similar gradient norm of 6.631955937915146e-07. Overall, the Quasi-Newton method consistently outperforms Newton's method in terms of the number of iterations required for convergence and the achieved gradient norm across different starting points.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
